{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few ideas so far:\n",
    "\n",
    "## 1 FFNN based\n",
    "- Train an nn on-top of sentence transformer to output logits / probs for score\n",
    "  - optimise loss function against given scores\n",
    "- Maybe experiment with longformer or othe large context window model instead of base sentence-transformer models as we might not be able to contain the entirety of the original text in smaller / short-passage model. \n",
    "- Try qlora technique to fine-tune llm on the prompt structure\n",
    "\n",
    "## 2 LLM based\n",
    "- Other approach might be to get a good language model to summarise the given prompt,\n",
    "- embed its output\n",
    "- embed the inference input\n",
    "- get the absolute difference between the two, using the llm output as a ground truth\n",
    "  - the closer the distance, the better the score we assign to the prompt\n",
    "- could also experiment with fine-tuning the llm against the train dataset, or other summarisation datasets\n",
    "  - would be a good opportunity to experiment with techniques such as peft and lora\n",
    "\n",
    "## 3 ensemble\n",
    "- Ensemble everything, maybe using somehing like rank averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/repositories/student-summary-evaluation/.env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chris/repositories/student-summary-evaluation/.env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332b9f7474a4461c98b36c4ec623c6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c9b3513934470499097c3043a11883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa2f2a35a949d5a6f16fb217224ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816032b389f4438e823335e66f7afc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d7809697284b1ea99f8a94843742d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LongformerTokenizerFast, LongformerModel\n",
    "import torch\n",
    "\n",
    "longformer = 'allenai/longformer-base-4096'\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(longformer)\n",
    "model = LongformerModel.from_pretrained(longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ' '.join(['hello world! '] * 1000)\n",
    "\n",
    "encoding = tokenizer.encode_plus(sample_text)\n",
    "input_ids = torch.tensor(encoding['input_ids']).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(encoding['attention_mask']).unsqueeze(0)\n",
    "\n",
    "global_attention_mask = torch.zeros(\n",
    "    input_ids.shape, dtype=torch.long, device=input_ids.device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, the `global_attention_mask` vectors needs to be set on the tokens contained in the question part of the input string,\n",
    "\n",
    "so for example say the quesition starts on character 21 and end on character 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx, end_idx = 21, 99\n",
    "\n",
    "question_start = encoding.char_to_token(start_idx)\n",
    "question_end = encoding.char_to_token(end_idx)\n",
    "\n",
    "# Attend to all question tokens\n",
    "global_attention_mask[:, [range(question_start, question_end)]] = 1\n",
    "global_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
