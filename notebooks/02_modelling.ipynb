{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few ideas so far:\n",
    "\n",
    "## 1 FFNN based\n",
    "- Train an nn on-top of sentence transformer to output logits / probs for score\n",
    "  - optimise loss function against given scores\n",
    "- Maybe experiment with longformer or othe large context window model instead of base sentence-transformer models as we might not be able to contain the entirety of the original text in smaller / short-passage model. \n",
    "- Longformer might need fine-tuning on sentence-pair datasets \n",
    "  - rationale https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers/64237402#64237402\n",
    "- Try qlora technique to fine-tune llm on the prompt structure\n",
    "\n",
    "## 2 LLM based\n",
    "- Other approach might be to get a good language model to summarise the given prompt,\n",
    "- embed its output\n",
    "- embed the inference input\n",
    "- get the absolute difference between the two, using the llm output as a ground truth\n",
    "  - the closer the distance, the better the score we assign to the prompt\n",
    "- could also experiment with fine-tuning the llm against the train dataset, or other summarisation datasets\n",
    "  - would be a good opportunity to experiment with techniques such as peft and lora\n",
    "\n",
    "\n",
    "## 3 ML\n",
    "- There are some correlations between wordcount and scoring values, so try to fit a standard regression model on these figures to output scores\n",
    "- Experiment with some more feature engineering, \n",
    "  - amount of unique words / n-grams\n",
    "  - amount of shared n-grams\n",
    "  - amount of stopwords in summarisation\n",
    "  - average + stddev sentence length in summarisation\n",
    "  - count of named entities shared in both\n",
    "  - etc ...\n",
    "\n",
    "## 4 ensemble\n",
    "- Ensemble everything, maybe using somehing like rank averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Longformer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/repositories/student-summary-evaluation/.env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chris/repositories/student-summary-evaluation/.env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerTokenizerFast, LongformerModel\n",
    "import torch\n",
    "\n",
    "longformer = 'allenai/longformer-base-4096'\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(longformer)\n",
    "model = LongformerModel.from_pretrained(longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ' '.join(['hello world! '] * 1000)\n",
    "\n",
    "encoding = tokenizer.encode_plus(sample_text)\n",
    "input_ids = torch.tensor(encoding['input_ids']).unsqueeze(0)\n",
    "\n",
    "attention_mask = torch.tensor(encoding['attention_mask']).unsqueeze(0)\n",
    "\n",
    "global_attention_mask = torch.zeros(\n",
    "    input_ids.shape, dtype=torch.long, device=input_ids.device\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, the `global_attention_mask` vectors needs to be set on the tokens contained in the question part of the input string,\n",
    "\n",
    "so for example say the quesition starts on character 21 and end on character 99.\n",
    "\n",
    "reference\n",
    "- https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb#scrollTo=tty8vuMBqI5L\n",
    "- https://huggingface.co/docs/transformers/model_doc/longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx, end_idx = 21, 99\n",
    "\n",
    "# translate indices from string to token seq\n",
    "question_start = encoding.char_to_token(start_idx)\n",
    "question_end = encoding.char_to_token(end_idx)\n",
    "\n",
    "# Attend to all question tokens\n",
    "global_attention_mask[:, [range(question_start, question_end)]] = 1\n",
    "global_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_output = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4002, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_output.shape, pooled_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output / embedding vector, could maybe take the hidden-state relating to the sentence start before the answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
