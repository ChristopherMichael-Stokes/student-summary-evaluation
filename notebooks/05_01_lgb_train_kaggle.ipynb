{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chris/repos/student-\n",
      "[nltk_data]     summary-evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chris/repos/student-\n",
      "[nltk_data]     summary-evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "download_dir = f'{os.getcwd()}/nltk_data'\n",
    "os.environ['NLTK_DATA'] = download_dir\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir=download_dir)\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('wordnet', download_dir=download_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=download_dir)\n",
    "nltk.download('universal_tagset', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "data_dir = Path('../data')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='inner', on='prompt_id')\n",
    "\n",
    "    if len(df) != len(summaries_df):\n",
    "        raise AssertionError('Could not match all prompt ids to a prompt')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "\n",
    "def nlp_splits(df: pd.DataFrame, column: str) -> None:\n",
    "    output = Parallel(n_jobs=4)(delayed(clear_stopwords)(df[column], idx) for idx in range(len(df)))\n",
    "\n",
    "    df[f'{column}_tokens'] = [part[0] for part in output]\n",
    "    df[f'{column}_no_stopwords'] = [part[1] for part in output]\n",
    "    df[f'{column}_lemmas'] = [part[2] for part in output]\n",
    "    df[f'{column}_bigram'] = [part[3] for part in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:50<00:00, 27.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = make_split(summaries_train, prompts_train)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in tqdm(text_columns):\n",
    "    nlp_splits(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "# Create n-gram based features\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_words'] = df_train.text_lemmas.str.len()\n",
    "df_train['unique_words'] = df_train.text_lemmas.apply(set).str.len()\n",
    "df_train['unique_ratio'] = df_train.unique_words / df_train.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_lengths'] = df_train.text_lemmas.apply(lambda x: [len(y) for y in x])\n",
    "df_train['word_len_avg'] = df_train.word_lengths.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_len_q10'] = df_train.word_lengths.apply(partial(np.percentile, q=10))\n",
    "df_train['word_len_q90'] = df_train.word_lengths.apply(partial(np.percentile, q=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pos_tag(df_train.text_lemmas[0], tagset='universal')\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(lambda: 0)\n",
    "for _, pos in x:\n",
    "    dd[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pos'] = df_train.text_lemmas.apply(partial(pos_tag, tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counts(tags):\n",
    "    dd = defaultdict(lambda: 0)\n",
    "    for _, pos in tags:\n",
    "        dd[pos] += 1\n",
    "    return dd\n",
    "\n",
    "df_train['pos_counts'] = df_train.pos.apply(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['verb_count'] = df_train.pos_counts.str['VERB'].replace(np.nan, 0)\n",
    "df_train['noun_count'] = df_train.pos_counts.str['NOUN'].replace(np.nan, 0)\n",
    "df_train['adv_count'] = df_train.pos_counts.str['ADV'].replace(np.nan, 0)\n",
    "df_train['adj_count'] = df_train.pos_counts.str['ADJ'].replace(np.nan, 0)\n",
    "df_train['det_count'] = df_train.pos_counts.str['DET'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>det_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>verb_count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805601</td>\n",
       "      <td>0.650115</td>\n",
       "      <td>0.642748</td>\n",
       "      <td>0.196229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noun_count</th>\n",
       "      <td>0.805601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566242</td>\n",
       "      <td>0.848588</td>\n",
       "      <td>0.197472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_count</th>\n",
       "      <td>0.650115</td>\n",
       "      <td>0.566242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.502692</td>\n",
       "      <td>0.156091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_count</th>\n",
       "      <td>0.642748</td>\n",
       "      <td>0.848588</td>\n",
       "      <td>0.502692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.132898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>det_count</th>\n",
       "      <td>0.196229</td>\n",
       "      <td>0.197472</td>\n",
       "      <td>0.156091</td>\n",
       "      <td>0.132898</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            verb_count  noun_count  adv_count  adj_count  det_count\n",
       "verb_count    1.000000    0.805601   0.650115   0.642748   0.196229\n",
       "noun_count    0.805601    1.000000   0.566242   0.848588   0.197472\n",
       "adv_count     0.650115    0.566242   1.000000   0.502692   0.156091\n",
       "adj_count     0.642748    0.848588   0.502692   1.000000   0.132898\n",
       "det_count     0.196229    0.197472   0.156091   0.132898   1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['verb_count','noun_count','adv_count','adj_count','det_count']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_lgb_kfold(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst\n",
    "\n",
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    \n",
    "    X_train = features[feature_names].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "    y_train = targets[target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "    train_errors = [calculate_errors(y_train, bst.predict(X_train))]\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors)\n",
    "\n",
    "    return train_metrics, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(f_cols, model_params):\n",
    "    metric_df_content, bst_content = train_lgb_kfold('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb_kfold('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nTrain MCRMSE:\\t   {mcrmse.iloc[0]}')\n",
    "    print(f'Validation MCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_title_unique_bigrams', 'prompt_question_unique_bigrams',\n",
       "       'prompt_text_unique_bigrams', 'text_unique_bigrams',\n",
       "       'text_bigram_overlap', 'question_bigram_overlap', 'text_bigram_ratio',\n",
       "       'text_bigram_diff', 'question_bigram_diff', 'text_bigram_exclusive',\n",
       "       'question_bigram_exclusive', 'n_words', 'unique_words', 'unique_ratio',\n",
       "       'word_len_avg', 'word_len_q10', 'word_len_q90', 'verb_count',\n",
       "       'noun_count', 'adv_count', 'adj_count', 'det_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.806390  0.459261  0.358195  train  content\n",
      "mean  0.759987  0.513684  0.402202    val  content\n",
      "mean  0.658127  0.604183  0.476375  train  wording\n",
      "mean  0.509102  0.706457  0.564051    val  wording\n",
      "std   0.006303  0.017028  0.013151  train  content\n",
      "std   0.015049  0.053489  0.036848    val  content\n",
      "std   0.030951  0.024802  0.018259  train  wording\n",
      "std   0.118705  0.122813  0.105093    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5317219417174222\n",
      "Validation MCRMSE: 0.6100704488099216\n",
      "\n",
      "    importance              feature\n",
      "0          406  text_bigram_overlap\n",
      "3          154              n_words\n",
      "5          103         word_len_avg\n",
      "1           94  text_unique_bigrams\n",
      "9           68           noun_count\n",
      "2           56         unique_ratio\n",
      "10          55            adv_count\n",
      "4           23         unique_words\n",
      "11          22            adj_count\n",
      "8           13           verb_count\n",
      "7            6         word_len_q90\n",
      "6            0         word_len_q10\n",
      "12           0            det_count\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'objective': 'fair', \n",
    "    'verbose': 0, \n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.08,\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 11,\n",
    "}\n",
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'verb_count','noun_count','adv_count','adj_count','det_count']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.791520  0.476565  0.370783  train  content\n",
      "mean  0.756455  0.517225  0.402603    val  content\n",
      "mean  0.621879  0.635708  0.500014  train  wording\n",
      "mean  0.498623  0.713939  0.568026    val  wording\n",
      "std   0.001926  0.013957  0.010144  train  content\n",
      "std   0.017666  0.052774  0.035961    val  content\n",
      "std   0.032610  0.031037  0.022116  train  wording\n",
      "std   0.133426  0.135442  0.113713    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5561367552440215\n",
      "Validation MCRMSE: 0.6155821194684965\n",
      "\n",
      "   importance              feature\n",
      "0         465  text_bigram_overlap\n",
      "1         427  text_unique_bigrams\n",
      "2         108         unique_ratio\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.790097  0.478199  0.371981  train  content\n",
      "mean  0.758234  0.515405  0.401389    val  content\n",
      "mean  0.616357  0.640318  0.503853  train  wording\n",
      "mean  0.510684  0.705167  0.559819    val  wording\n",
      "std   0.002559  0.014749  0.010732  train  content\n",
      "std   0.017286  0.053294  0.036332    val  content\n",
      "std   0.033239  0.031048  0.021957  train  wording\n",
      "std   0.128460  0.131434  0.109709    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.559258697469001\n",
      "Validation MCRMSE: 0.6102856988744446\n",
      "\n",
      "   importance              feature\n",
      "0         527  text_bigram_overlap\n",
      "1         473  text_unique_bigrams\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.804425  0.461585  0.359985  train  content\n",
      "mean  0.765076  0.508234  0.397803    val  content\n",
      "mean  0.649260  0.612031  0.482072  train  wording\n",
      "mean  0.515658  0.701880  0.559380    val  wording\n",
      "std   0.006252  0.017009  0.012971  train  content\n",
      "std   0.013946  0.052668  0.036411    val  content\n",
      "std   0.032625  0.028631  0.021088  train  wording\n",
      "std   0.117498  0.122713  0.104864    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.536808078044499\n",
      "Validation MCRMSE: 0.6050568632489971\n",
      "\n",
      "   importance              feature\n",
      "0         403  text_bigram_overlap\n",
      "3         218              n_words\n",
      "1         137  text_unique_bigrams\n",
      "5         132         word_len_avg\n",
      "2          59         unique_ratio\n",
      "4          39         unique_words\n",
      "7          11         word_len_q90\n",
      "6           1         word_len_q10\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "\tContent scores\n",
      "         r2      rmse      mae\n",
      "0  0.785977  0.482748  0.37519\n",
      "\n",
      "-----------------------------------\n",
      "\tWording scores\n",
      "         r2      rmse       mae\n",
      "0  0.611153  0.646009  0.508004\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{\"-\"*35}\\n\\tContent scores')\n",
    "pprint(metric_df_content)\n",
    "print(f'\\n{\"-\"*35}\\n\\tWording scores')\n",
    "pprint(metric_df_wording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full data MCRMSE: ')\n",
    "(metric_df_content.rmse + metric_df_wording.rmse) / 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
