{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "download_dir = f'{os.getcwd()}/nltk_data'\n",
    "os.environ['NLTK_DATA'] = download_dir\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir=download_dir)\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('wordnet', download_dir=download_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=download_dir)\n",
    "nltk.download('universal_tagset', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "data_dir = Path('../data')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='inner', on='prompt_id')\n",
    "\n",
    "    if len(df) != len(summaries_df):\n",
    "        raise AssertionError('Could not match all prompt ids to a prompt')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "\n",
    "def nlp_splits(df: pd.DataFrame, column: str) -> None:\n",
    "    output = Parallel(n_jobs=4)(delayed(clear_stopwords)(df[column], idx) for idx in range(len(df)))\n",
    "\n",
    "    df[f'{column}_tokens'] = [part[0] for part in output]\n",
    "    df[f'{column}_no_stopwords'] = [part[1] for part in output]\n",
    "    df[f'{column}_lemmas'] = [part[2] for part in output]\n",
    "    df[f'{column}_bigram'] = [part[3] for part in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = make_split(summaries_train, prompts_train)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in tqdm(text_columns):\n",
    "    nlp_splits(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "# Create n-gram based features\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_words'] = df_train.text_lemmas.str.len()\n",
    "df_train['unique_words'] = df_train.text_lemmas.apply(set).str.len()\n",
    "df_train['unique_ratio'] = df_train.unique_words / df_train.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_lengths'] = df_train.text_lemmas.apply(lambda x: [len(y) for y in x])\n",
    "df_train['word_len_avg'] = df_train.word_lengths.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_len_q10'] = df_train.word_lengths.apply(partial(np.percentile, q=10))\n",
    "df_train['word_len_q90'] = df_train.word_lengths.apply(partial(np.percentile, q=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pos_tag(df_train.text_lemmas[0], tagset='universal')\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(lambda: 0)\n",
    "for _, pos in x:\n",
    "    dd[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pos'] = df_train.text_lemmas.apply(partial(pos_tag, tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counts(tags):\n",
    "    dd = defaultdict(lambda: 0)\n",
    "    for _, pos in tags:\n",
    "        dd[pos] += 1\n",
    "    return dd\n",
    "\n",
    "df_train['pos_counts'] = df_train.pos.apply(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['verb_count'] = df_train.pos_counts.str['VERB'].replace(np.nan, 0)\n",
    "df_train['noun_count'] = df_train.pos_counts.str['NOUN'].replace(np.nan, 0)\n",
    "df_train['adv_count'] = df_train.pos_counts.str['ADV'].replace(np.nan, 0)\n",
    "df_train['adj_count'] = df_train.pos_counts.str['ADJ'].replace(np.nan, 0)\n",
    "df_train['det_count'] = df_train.pos_counts.str['DET'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['verb_count','noun_count','adv_count','adj_count','det_count']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_lgb_kfold(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst\n",
    "\n",
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    \n",
    "    X_train = features[feature_names].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "    y_train = targets[target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "    train_errors = [calculate_errors(y_train, bst.predict(X_train))]\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors)\n",
    "\n",
    "    return train_metrics, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(f_cols, model_params):\n",
    "    metric_df_content, bst_content = train_lgb_kfold('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb_kfold('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nTrain MCRMSE:\\t   {mcrmse.iloc[0]}')\n",
    "    print(f'Validation MCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'objective': 'fair', \n",
    "    'verbose': 0, \n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.08,\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 11,\n",
    "}\n",
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'verb_count','noun_count','adv_count','adj_count','det_count']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"-\"*35}\\n\\tContent scores')\n",
    "pprint(metric_df_content)\n",
    "print(f'\\n{\"-\"*35}\\n\\tWording scores')\n",
    "pprint(metric_df_wording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full data MCRMSE: ')\n",
    "(metric_df_content.rmse + metric_df_wording.rmse) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
