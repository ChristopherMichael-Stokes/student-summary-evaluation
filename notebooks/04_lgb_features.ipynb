{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/chris/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'\n",
    "\n",
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='inner', on='prompt_id')\n",
    "\n",
    "    if len(df) != len(summaries_df):\n",
    "        raise AssertionError('Could not match all prompt ids to a prompt')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = make_split(summaries_train, prompts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>Summarize how the Third Wave developed over su...</td>\n",
       "      <td>The Third Wave</td>\n",
       "      <td>Background \r\n",
       "The Third Wave experiment took pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "\n",
       "    content   wording                                    prompt_question  \\\n",
       "0  0.205683  0.380538  Summarize how the Third Wave developed over su...   \n",
       "\n",
       "     prompt_title                                        prompt_text  \n",
       "0  The Third Wave  Background \n",
       "The Third Wave experiment took pl...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "    # tri_gram = set(ngrams(lemmas, 3))\n",
    "    # four_gram = set(ngrams(lemmas, 4))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram #, tri_gram, four_gram\n",
    "\n",
    "def nlp_splits(df: pd.DataFrame, column: str) -> None:\n",
    "    output = Parallel(n_jobs=4, backend='multiprocessing')(delayed(clear_stopwords)(df[column], idx) for idx in range(len(df)))\n",
    "\n",
    "    df[f'{column}_tokens'] = [part[0] for part in output]\n",
    "    df[f'{column}_no_stopwords'] = [part[1] for part in output]\n",
    "    df[f'{column}_lemmas'] = [part[2] for part in output]\n",
    "    df[f'{column}_bigram'] = [part[3] for part in output]\n",
    "\n",
    "x = clear_stopwords(df.text, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in text_columns:\n",
    "    nlp_splits(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / ((df_train.text_unique_bigrams + df_train.prompt_text_unique_bigrams) / 2)\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / ((df_train.text_unique_bigrams + df_train.prompt_text_unique_bigrams) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_33678_row0_col0, #T_33678_row1_col1, #T_33678_row2_col2, #T_33678_row3_col3, #T_33678_row4_col4, #T_33678_row5_col5, #T_33678_row6_col6, #T_33678_row7_col7, #T_33678_row8_col8, #T_33678_row9_col9, #T_33678_row10_col10, #T_33678_row11_col11, #T_33678_row12_col12 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col1 {\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col2 {\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col3, #T_33678_row3_col1 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col4 {\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col5 {\n",
       "  background-color: #dd5f4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col6, #T_33678_row2_col7, #T_33678_row4_col10 {\n",
       "  background-color: #dbdcde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row0_col7, #T_33678_row4_col5 {\n",
       "  background-color: #d1dae9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row0_col8 {\n",
       "  background-color: #de614d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col9, #T_33678_row2_col10, #T_33678_row4_col7 {\n",
       "  background-color: #dedcdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row0_col10 {\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row0_col11 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row0_col12, #T_33678_row11_col9 {\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row1_col0 {\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row1_col2 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row1_col3, #T_33678_row11_col6 {\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row1_col4, #T_33678_row7_col3 {\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row1_col5, #T_33678_row1_col12 {\n",
       "  background-color: #f6a586;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col6 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col7, #T_33678_row9_col1, #T_33678_row10_col5 {\n",
       "  background-color: #d9dce1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col8 {\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col9 {\n",
       "  background-color: #f7ba9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col10, #T_33678_row6_col5, #T_33678_row7_col11 {\n",
       "  background-color: #e0dbd8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row1_col11, #T_33678_row3_col9 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col0 {\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col1, #T_33678_row4_col0, #T_33678_row8_col3, #T_33678_row10_col1 {\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col3, #T_33678_row3_col2, #T_33678_row5_col12, #T_33678_row12_col5 {\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row2_col4, #T_33678_row12_col2 {\n",
       "  background-color: #b7cff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col5, #T_33678_row3_col5 {\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col6 {\n",
       "  background-color: #ecd3c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col8, #T_33678_row6_col8 {\n",
       "  background-color: #d2dbe8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col9, #T_33678_row6_col4 {\n",
       "  background-color: #cbd8ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col11 {\n",
       "  background-color: #a1c0ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row2_col12, #T_33678_row3_col12 {\n",
       "  background-color: #e7d7ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col0, #T_33678_row9_col12 {\n",
       "  background-color: #9fbfff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col4 {\n",
       "  background-color: #c7d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col6 {\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col7 {\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col8 {\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col10 {\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row3_col11, #T_33678_row9_col0 {\n",
       "  background-color: #9dbdff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col1 {\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row4_col2, #T_33678_row10_col11 {\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col3 {\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col6 {\n",
       "  background-color: #f5c0a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col8, #T_33678_row5_col2, #T_33678_row7_col8 {\n",
       "  background-color: #9ebeff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col9 {\n",
       "  background-color: #b3cdfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col11 {\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row4_col12 {\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col0, #T_33678_row8_col0 {\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row5_col1 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col3, #T_33678_row9_col8 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col4 {\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col6 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col7, #T_33678_row8_col7 {\n",
       "  background-color: #c9d7f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col8 {\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row5_col9 {\n",
       "  background-color: #c3d5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col10, #T_33678_row8_col10 {\n",
       "  background-color: #edd1c2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row5_col11 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row6_col0 {\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row6_col1, #T_33678_row6_col9, #T_33678_row6_col11, #T_33678_row7_col10, #T_33678_row9_col4, #T_33678_row9_col6, #T_33678_row10_col7, #T_33678_row11_col0, #T_33678_row11_col2, #T_33678_row11_col3, #T_33678_row11_col5, #T_33678_row11_col8, #T_33678_row11_col12 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row6_col2 {\n",
       "  background-color: #93b5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row6_col3 {\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row6_col7, #T_33678_row7_col6, #T_33678_row9_col10, #T_33678_row10_col9, #T_33678_row11_col10 {\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row6_col10, #T_33678_row7_col9, #T_33678_row9_col7, #T_33678_row10_col6, #T_33678_row11_col7 {\n",
       "  background-color: #efcfbf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row6_col12 {\n",
       "  background-color: #d5dbe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row7_col0, #T_33678_row7_col1 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row7_col2 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row7_col4 {\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row7_col5 {\n",
       "  background-color: #a5c3fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row7_col12, #T_33678_row10_col4 {\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row8_col1 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row8_col2 {\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row8_col4 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row8_col5, #T_33678_row8_col12, #T_33678_row12_col8 {\n",
       "  background-color: #bb1b2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row8_col6 {\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row8_col9 {\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row8_col11, #T_33678_row9_col2 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row9_col3 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row9_col5 {\n",
       "  background-color: #9bbcff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row9_col11, #T_33678_row12_col0 {\n",
       "  background-color: #e57058;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row10_col0 {\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row10_col2 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row10_col3 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row10_col8 {\n",
       "  background-color: #d7dce3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row10_col12, #T_33678_row12_col6 {\n",
       "  background-color: #edd2c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row11_col1 {\n",
       "  background-color: #86a9fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row11_col4 {\n",
       "  background-color: #6384eb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row12_col1 {\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row12_col3 {\n",
       "  background-color: #bbd1f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row12_col4 {\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_33678_row12_col7 {\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row12_col9 {\n",
       "  background-color: #cad8ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row12_col10 {\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_33678_row12_col11 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_33678\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_33678_level0_col0\" class=\"col_heading level0 col0\" >content</th>\n",
       "      <th id=\"T_33678_level0_col1\" class=\"col_heading level0 col1\" >wording</th>\n",
       "      <th id=\"T_33678_level0_col2\" class=\"col_heading level0 col2\" >prompt_title_unique_bigrams</th>\n",
       "      <th id=\"T_33678_level0_col3\" class=\"col_heading level0 col3\" >prompt_question_unique_bigrams</th>\n",
       "      <th id=\"T_33678_level0_col4\" class=\"col_heading level0 col4\" >prompt_text_unique_bigrams</th>\n",
       "      <th id=\"T_33678_level0_col5\" class=\"col_heading level0 col5\" >text_unique_bigrams</th>\n",
       "      <th id=\"T_33678_level0_col6\" class=\"col_heading level0 col6\" >text_bigram_overlap</th>\n",
       "      <th id=\"T_33678_level0_col7\" class=\"col_heading level0 col7\" >question_bigram_overlap</th>\n",
       "      <th id=\"T_33678_level0_col8\" class=\"col_heading level0 col8\" >text_bigram_ratio</th>\n",
       "      <th id=\"T_33678_level0_col9\" class=\"col_heading level0 col9\" >text_bigram_diff</th>\n",
       "      <th id=\"T_33678_level0_col10\" class=\"col_heading level0 col10\" >question_bigram_diff</th>\n",
       "      <th id=\"T_33678_level0_col11\" class=\"col_heading level0 col11\" >text_bigram_exclusive</th>\n",
       "      <th id=\"T_33678_level0_col12\" class=\"col_heading level0 col12\" >question_bigram_exclusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row0\" class=\"row_heading level0 row0\" >content</th>\n",
       "      <td id=\"T_33678_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_33678_row0_col1\" class=\"data row0 col1\" >0.751380</td>\n",
       "      <td id=\"T_33678_row0_col2\" class=\"data row0 col2\" >0.052423</td>\n",
       "      <td id=\"T_33678_row0_col3\" class=\"data row0 col3\" >0.026536</td>\n",
       "      <td id=\"T_33678_row0_col4\" class=\"data row0 col4\" >-0.030230</td>\n",
       "      <td id=\"T_33678_row0_col5\" class=\"data row0 col5\" >0.793555</td>\n",
       "      <td id=\"T_33678_row0_col6\" class=\"data row0 col6\" >-0.011486</td>\n",
       "      <td id=\"T_33678_row0_col7\" class=\"data row0 col7\" >-0.090704</td>\n",
       "      <td id=\"T_33678_row0_col8\" class=\"data row0 col8\" >0.795084</td>\n",
       "      <td id=\"T_33678_row0_col9\" class=\"data row0 col9\" >0.011486</td>\n",
       "      <td id=\"T_33678_row0_col10\" class=\"data row0 col10\" >0.090704</td>\n",
       "      <td id=\"T_33678_row0_col11\" class=\"data row0 col11\" >-0.399939</td>\n",
       "      <td id=\"T_33678_row0_col12\" class=\"data row0 col12\" >0.779024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row1\" class=\"row_heading level0 row1\" >wording</th>\n",
       "      <td id=\"T_33678_row1_col0\" class=\"data row1 col0\" >0.751380</td>\n",
       "      <td id=\"T_33678_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_33678_row1_col2\" class=\"data row1 col2\" >0.026611</td>\n",
       "      <td id=\"T_33678_row1_col3\" class=\"data row1 col3\" >-0.056031</td>\n",
       "      <td id=\"T_33678_row1_col4\" class=\"data row1 col4\" >-0.125305</td>\n",
       "      <td id=\"T_33678_row1_col5\" class=\"data row1 col5\" >0.529123</td>\n",
       "      <td id=\"T_33678_row1_col6\" class=\"data row1 col6\" >-0.319900</td>\n",
       "      <td id=\"T_33678_row1_col7\" class=\"data row1 col7\" >-0.028342</td>\n",
       "      <td id=\"T_33678_row1_col8\" class=\"data row1 col8\" >0.559532</td>\n",
       "      <td id=\"T_33678_row1_col9\" class=\"data row1 col9\" >0.319900</td>\n",
       "      <td id=\"T_33678_row1_col10\" class=\"data row1 col10\" >0.028342</td>\n",
       "      <td id=\"T_33678_row1_col11\" class=\"data row1 col11\" >-0.014476</td>\n",
       "      <td id=\"T_33678_row1_col12\" class=\"data row1 col12\" >0.542503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row2\" class=\"row_heading level0 row2\" >prompt_title_unique_bigrams</th>\n",
       "      <td id=\"T_33678_row2_col0\" class=\"data row2 col0\" >0.052423</td>\n",
       "      <td id=\"T_33678_row2_col1\" class=\"data row2 col1\" >0.026611</td>\n",
       "      <td id=\"T_33678_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_33678_row2_col3\" class=\"data row2 col3\" >0.940945</td>\n",
       "      <td id=\"T_33678_row2_col4\" class=\"data row2 col4\" >0.198507</td>\n",
       "      <td id=\"T_33678_row2_col5\" class=\"data row2 col5\" >0.162399</td>\n",
       "      <td id=\"T_33678_row2_col6\" class=\"data row2 col6\" >0.125865</td>\n",
       "      <td id=\"T_33678_row2_col7\" class=\"data row2 col7\" >-0.014407</td>\n",
       "      <td id=\"T_33678_row2_col8\" class=\"data row2 col8\" >0.114988</td>\n",
       "      <td id=\"T_33678_row2_col9\" class=\"data row2 col9\" >-0.125865</td>\n",
       "      <td id=\"T_33678_row2_col10\" class=\"data row2 col10\" >0.014407</td>\n",
       "      <td id=\"T_33678_row2_col11\" class=\"data row2 col11\" >-0.195834</td>\n",
       "      <td id=\"T_33678_row2_col12\" class=\"data row2 col12\" >0.251099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row3\" class=\"row_heading level0 row3\" >prompt_question_unique_bigrams</th>\n",
       "      <td id=\"T_33678_row3_col0\" class=\"data row3 col0\" >0.026536</td>\n",
       "      <td id=\"T_33678_row3_col1\" class=\"data row3 col1\" >-0.056031</td>\n",
       "      <td id=\"T_33678_row3_col2\" class=\"data row3 col2\" >0.940945</td>\n",
       "      <td id=\"T_33678_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_33678_row3_col4\" class=\"data row3 col4\" >0.261953</td>\n",
       "      <td id=\"T_33678_row3_col5\" class=\"data row3 col5\" >0.162973</td>\n",
       "      <td id=\"T_33678_row3_col6\" class=\"data row3 col6\" >0.178714</td>\n",
       "      <td id=\"T_33678_row3_col7\" class=\"data row3 col7\" >-0.073393</td>\n",
       "      <td id=\"T_33678_row3_col8\" class=\"data row3 col8\" >0.101703</td>\n",
       "      <td id=\"T_33678_row3_col9\" class=\"data row3 col9\" >-0.178714</td>\n",
       "      <td id=\"T_33678_row3_col10\" class=\"data row3 col10\" >0.073393</td>\n",
       "      <td id=\"T_33678_row3_col11\" class=\"data row3 col11\" >-0.218553</td>\n",
       "      <td id=\"T_33678_row3_col12\" class=\"data row3 col12\" >0.251833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row4\" class=\"row_heading level0 row4\" >prompt_text_unique_bigrams</th>\n",
       "      <td id=\"T_33678_row4_col0\" class=\"data row4 col0\" >-0.030230</td>\n",
       "      <td id=\"T_33678_row4_col1\" class=\"data row4 col1\" >-0.125305</td>\n",
       "      <td id=\"T_33678_row4_col2\" class=\"data row4 col2\" >0.198507</td>\n",
       "      <td id=\"T_33678_row4_col3\" class=\"data row4 col3\" >0.261953</td>\n",
       "      <td id=\"T_33678_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_33678_row4_col5\" class=\"data row4 col5\" >0.087776</td>\n",
       "      <td id=\"T_33678_row4_col6\" class=\"data row4 col6\" >0.277909</td>\n",
       "      <td id=\"T_33678_row4_col7\" class=\"data row4 col7\" >0.015343</td>\n",
       "      <td id=\"T_33678_row4_col8\" class=\"data row4 col8\" >-0.141215</td>\n",
       "      <td id=\"T_33678_row4_col9\" class=\"data row4 col9\" >-0.277909</td>\n",
       "      <td id=\"T_33678_row4_col10\" class=\"data row4 col10\" >-0.015343</td>\n",
       "      <td id=\"T_33678_row4_col11\" class=\"data row4 col11\" >-0.108869</td>\n",
       "      <td id=\"T_33678_row4_col12\" class=\"data row4 col12\" >-0.184709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row5\" class=\"row_heading level0 row5\" >text_unique_bigrams</th>\n",
       "      <td id=\"T_33678_row5_col0\" class=\"data row5 col0\" >0.793555</td>\n",
       "      <td id=\"T_33678_row5_col1\" class=\"data row5 col1\" >0.529123</td>\n",
       "      <td id=\"T_33678_row5_col2\" class=\"data row5 col2\" >0.162399</td>\n",
       "      <td id=\"T_33678_row5_col3\" class=\"data row5 col3\" >0.162973</td>\n",
       "      <td id=\"T_33678_row5_col4\" class=\"data row5 col4\" >0.087776</td>\n",
       "      <td id=\"T_33678_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_33678_row5_col6\" class=\"data row5 col6\" >0.186255</td>\n",
       "      <td id=\"T_33678_row5_col7\" class=\"data row5 col7\" >-0.141032</td>\n",
       "      <td id=\"T_33678_row5_col8\" class=\"data row5 col8\" >0.961015</td>\n",
       "      <td id=\"T_33678_row5_col9\" class=\"data row5 col9\" >-0.186255</td>\n",
       "      <td id=\"T_33678_row5_col10\" class=\"data row5 col10\" >0.141032</td>\n",
       "      <td id=\"T_33678_row5_col11\" class=\"data row5 col11\" >-0.675624</td>\n",
       "      <td id=\"T_33678_row5_col12\" class=\"data row5 col12\" >0.918799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row6\" class=\"row_heading level0 row6\" >text_bigram_overlap</th>\n",
       "      <td id=\"T_33678_row6_col0\" class=\"data row6 col0\" >-0.011486</td>\n",
       "      <td id=\"T_33678_row6_col1\" class=\"data row6 col1\" >-0.319900</td>\n",
       "      <td id=\"T_33678_row6_col2\" class=\"data row6 col2\" >0.125865</td>\n",
       "      <td id=\"T_33678_row6_col3\" class=\"data row6 col3\" >0.178714</td>\n",
       "      <td id=\"T_33678_row6_col4\" class=\"data row6 col4\" >0.277909</td>\n",
       "      <td id=\"T_33678_row6_col5\" class=\"data row6 col5\" >0.186255</td>\n",
       "      <td id=\"T_33678_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_33678_row6_col7\" class=\"data row6 col7\" >-0.157848</td>\n",
       "      <td id=\"T_33678_row6_col8\" class=\"data row6 col8\" >0.117397</td>\n",
       "      <td id=\"T_33678_row6_col9\" class=\"data row6 col9\" >-1.000000</td>\n",
       "      <td id=\"T_33678_row6_col10\" class=\"data row6 col10\" >0.157848</td>\n",
       "      <td id=\"T_33678_row6_col11\" class=\"data row6 col11\" >-0.727320</td>\n",
       "      <td id=\"T_33678_row6_col12\" class=\"data row6 col12\" >0.137636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row7\" class=\"row_heading level0 row7\" >question_bigram_overlap</th>\n",
       "      <td id=\"T_33678_row7_col0\" class=\"data row7 col0\" >-0.090704</td>\n",
       "      <td id=\"T_33678_row7_col1\" class=\"data row7 col1\" >-0.028342</td>\n",
       "      <td id=\"T_33678_row7_col2\" class=\"data row7 col2\" >-0.014407</td>\n",
       "      <td id=\"T_33678_row7_col3\" class=\"data row7 col3\" >-0.073393</td>\n",
       "      <td id=\"T_33678_row7_col4\" class=\"data row7 col4\" >0.015343</td>\n",
       "      <td id=\"T_33678_row7_col5\" class=\"data row7 col5\" >-0.141032</td>\n",
       "      <td id=\"T_33678_row7_col6\" class=\"data row7 col6\" >-0.157848</td>\n",
       "      <td id=\"T_33678_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_33678_row7_col8\" class=\"data row7 col8\" >-0.146373</td>\n",
       "      <td id=\"T_33678_row7_col9\" class=\"data row7 col9\" >0.157848</td>\n",
       "      <td id=\"T_33678_row7_col10\" class=\"data row7 col10\" >-1.000000</td>\n",
       "      <td id=\"T_33678_row7_col11\" class=\"data row7 col11\" >0.160251</td>\n",
       "      <td id=\"T_33678_row7_col12\" class=\"data row7 col12\" >-0.296151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row8\" class=\"row_heading level0 row8\" >text_bigram_ratio</th>\n",
       "      <td id=\"T_33678_row8_col0\" class=\"data row8 col0\" >0.795084</td>\n",
       "      <td id=\"T_33678_row8_col1\" class=\"data row8 col1\" >0.559532</td>\n",
       "      <td id=\"T_33678_row8_col2\" class=\"data row8 col2\" >0.114988</td>\n",
       "      <td id=\"T_33678_row8_col3\" class=\"data row8 col3\" >0.101703</td>\n",
       "      <td id=\"T_33678_row8_col4\" class=\"data row8 col4\" >-0.141215</td>\n",
       "      <td id=\"T_33678_row8_col5\" class=\"data row8 col5\" >0.961015</td>\n",
       "      <td id=\"T_33678_row8_col6\" class=\"data row8 col6\" >0.117397</td>\n",
       "      <td id=\"T_33678_row8_col7\" class=\"data row8 col7\" >-0.146373</td>\n",
       "      <td id=\"T_33678_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "      <td id=\"T_33678_row8_col9\" class=\"data row8 col9\" >-0.117397</td>\n",
       "      <td id=\"T_33678_row8_col10\" class=\"data row8 col10\" >0.146373</td>\n",
       "      <td id=\"T_33678_row8_col11\" class=\"data row8 col11\" >-0.631491</td>\n",
       "      <td id=\"T_33678_row8_col12\" class=\"data row8 col12\" >0.962103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row9\" class=\"row_heading level0 row9\" >text_bigram_diff</th>\n",
       "      <td id=\"T_33678_row9_col0\" class=\"data row9 col0\" >0.011486</td>\n",
       "      <td id=\"T_33678_row9_col1\" class=\"data row9 col1\" >0.319900</td>\n",
       "      <td id=\"T_33678_row9_col2\" class=\"data row9 col2\" >-0.125865</td>\n",
       "      <td id=\"T_33678_row9_col3\" class=\"data row9 col3\" >-0.178714</td>\n",
       "      <td id=\"T_33678_row9_col4\" class=\"data row9 col4\" >-0.277909</td>\n",
       "      <td id=\"T_33678_row9_col5\" class=\"data row9 col5\" >-0.186255</td>\n",
       "      <td id=\"T_33678_row9_col6\" class=\"data row9 col6\" >-1.000000</td>\n",
       "      <td id=\"T_33678_row9_col7\" class=\"data row9 col7\" >0.157848</td>\n",
       "      <td id=\"T_33678_row9_col8\" class=\"data row9 col8\" >-0.117397</td>\n",
       "      <td id=\"T_33678_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "      <td id=\"T_33678_row9_col10\" class=\"data row9 col10\" >-0.157848</td>\n",
       "      <td id=\"T_33678_row9_col11\" class=\"data row9 col11\" >0.727320</td>\n",
       "      <td id=\"T_33678_row9_col12\" class=\"data row9 col12\" >-0.137636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row10\" class=\"row_heading level0 row10\" >question_bigram_diff</th>\n",
       "      <td id=\"T_33678_row10_col0\" class=\"data row10 col0\" >0.090704</td>\n",
       "      <td id=\"T_33678_row10_col1\" class=\"data row10 col1\" >0.028342</td>\n",
       "      <td id=\"T_33678_row10_col2\" class=\"data row10 col2\" >0.014407</td>\n",
       "      <td id=\"T_33678_row10_col3\" class=\"data row10 col3\" >0.073393</td>\n",
       "      <td id=\"T_33678_row10_col4\" class=\"data row10 col4\" >-0.015343</td>\n",
       "      <td id=\"T_33678_row10_col5\" class=\"data row10 col5\" >0.141032</td>\n",
       "      <td id=\"T_33678_row10_col6\" class=\"data row10 col6\" >0.157848</td>\n",
       "      <td id=\"T_33678_row10_col7\" class=\"data row10 col7\" >-1.000000</td>\n",
       "      <td id=\"T_33678_row10_col8\" class=\"data row10 col8\" >0.146373</td>\n",
       "      <td id=\"T_33678_row10_col9\" class=\"data row10 col9\" >-0.157848</td>\n",
       "      <td id=\"T_33678_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "      <td id=\"T_33678_row10_col11\" class=\"data row10 col11\" >-0.160251</td>\n",
       "      <td id=\"T_33678_row10_col12\" class=\"data row10 col12\" >0.296151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row11\" class=\"row_heading level0 row11\" >text_bigram_exclusive</th>\n",
       "      <td id=\"T_33678_row11_col0\" class=\"data row11 col0\" >-0.399939</td>\n",
       "      <td id=\"T_33678_row11_col1\" class=\"data row11 col1\" >-0.014476</td>\n",
       "      <td id=\"T_33678_row11_col2\" class=\"data row11 col2\" >-0.195834</td>\n",
       "      <td id=\"T_33678_row11_col3\" class=\"data row11 col3\" >-0.218553</td>\n",
       "      <td id=\"T_33678_row11_col4\" class=\"data row11 col4\" >-0.108869</td>\n",
       "      <td id=\"T_33678_row11_col5\" class=\"data row11 col5\" >-0.675624</td>\n",
       "      <td id=\"T_33678_row11_col6\" class=\"data row11 col6\" >-0.727320</td>\n",
       "      <td id=\"T_33678_row11_col7\" class=\"data row11 col7\" >0.160251</td>\n",
       "      <td id=\"T_33678_row11_col8\" class=\"data row11 col8\" >-0.631491</td>\n",
       "      <td id=\"T_33678_row11_col9\" class=\"data row11 col9\" >0.727320</td>\n",
       "      <td id=\"T_33678_row11_col10\" class=\"data row11 col10\" >-0.160251</td>\n",
       "      <td id=\"T_33678_row11_col11\" class=\"data row11 col11\" >1.000000</td>\n",
       "      <td id=\"T_33678_row11_col12\" class=\"data row11 col12\" >-0.633780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_33678_level0_row12\" class=\"row_heading level0 row12\" >question_bigram_exclusive</th>\n",
       "      <td id=\"T_33678_row12_col0\" class=\"data row12 col0\" >0.779024</td>\n",
       "      <td id=\"T_33678_row12_col1\" class=\"data row12 col1\" >0.542503</td>\n",
       "      <td id=\"T_33678_row12_col2\" class=\"data row12 col2\" >0.251099</td>\n",
       "      <td id=\"T_33678_row12_col3\" class=\"data row12 col3\" >0.251833</td>\n",
       "      <td id=\"T_33678_row12_col4\" class=\"data row12 col4\" >-0.184709</td>\n",
       "      <td id=\"T_33678_row12_col5\" class=\"data row12 col5\" >0.918799</td>\n",
       "      <td id=\"T_33678_row12_col6\" class=\"data row12 col6\" >0.137636</td>\n",
       "      <td id=\"T_33678_row12_col7\" class=\"data row12 col7\" >-0.296151</td>\n",
       "      <td id=\"T_33678_row12_col8\" class=\"data row12 col8\" >0.962103</td>\n",
       "      <td id=\"T_33678_row12_col9\" class=\"data row12 col9\" >-0.137636</td>\n",
       "      <td id=\"T_33678_row12_col10\" class=\"data row12 col10\" >0.296151</td>\n",
       "      <td id=\"T_33678_row12_col11\" class=\"data row12 col11\" >-0.633780</td>\n",
       "      <td id=\"T_33678_row12_col12\" class=\"data row12 col12\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc270e36190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "corr = numeric_features.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>prompt_title_unique_bigrams</th>\n",
       "      <th>prompt_question_unique_bigrams</th>\n",
       "      <th>prompt_text_unique_bigrams</th>\n",
       "      <th>text_unique_bigrams</th>\n",
       "      <th>text_bigram_overlap</th>\n",
       "      <th>question_bigram_overlap</th>\n",
       "      <th>text_bigram_ratio</th>\n",
       "      <th>text_bigram_diff</th>\n",
       "      <th>question_bigram_diff</th>\n",
       "      <th>text_bigram_exclusive</th>\n",
       "      <th>question_bigram_exclusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "      <td>7165.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.014853</td>\n",
       "      <td>-0.063072</td>\n",
       "      <td>0.993301</td>\n",
       "      <td>10.511375</td>\n",
       "      <td>323.413957</td>\n",
       "      <td>34.973761</td>\n",
       "      <td>0.256570</td>\n",
       "      <td>0.045554</td>\n",
       "      <td>0.110370</td>\n",
       "      <td>0.743430</td>\n",
       "      <td>0.954446</td>\n",
       "      <td>1.895557</td>\n",
       "      <td>0.234856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.043569</td>\n",
       "      <td>1.036048</td>\n",
       "      <td>0.753336</td>\n",
       "      <td>3.198101</td>\n",
       "      <td>62.490900</td>\n",
       "      <td>25.553950</td>\n",
       "      <td>0.245381</td>\n",
       "      <td>0.064807</td>\n",
       "      <td>0.081772</td>\n",
       "      <td>0.245381</td>\n",
       "      <td>0.064807</td>\n",
       "      <td>0.130174</td>\n",
       "      <td>0.114135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.729859</td>\n",
       "      <td>-1.962614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.039867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.799545</td>\n",
       "      <td>-0.872720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058419</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.855626</td>\n",
       "      <td>0.155477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.093814</td>\n",
       "      <td>-0.081769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.085911</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>1.944637</td>\n",
       "      <td>0.206186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.499660</td>\n",
       "      <td>0.503833</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.134021</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.985765</td>\n",
       "      <td>0.281437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.900326</td>\n",
       "      <td>4.310693</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.093284</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.088000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           content      wording  prompt_title_unique_bigrams  \\\n",
       "count  7165.000000  7165.000000                  7165.000000   \n",
       "mean     -0.014853    -0.063072                     0.993301   \n",
       "std       1.043569     1.036048                     0.753336   \n",
       "min      -1.729859    -1.962614                     0.000000   \n",
       "25%      -0.799545    -0.872720                     0.000000   \n",
       "50%      -0.093814    -0.081769                     1.000000   \n",
       "75%       0.499660     0.503833                     2.000000   \n",
       "max       3.900326     4.310693                     2.000000   \n",
       "\n",
       "       prompt_question_unique_bigrams  prompt_text_unique_bigrams  \\\n",
       "count                     7165.000000                 7165.000000   \n",
       "mean                        10.511375                  323.413957   \n",
       "std                          3.198101                   62.490900   \n",
       "min                          7.000000                  268.000000   \n",
       "25%                          7.000000                  268.000000   \n",
       "50%                         11.000000                  300.000000   \n",
       "75%                         15.000000                  422.000000   \n",
       "max                         15.000000                  422.000000   \n",
       "\n",
       "       text_unique_bigrams  text_bigram_overlap  question_bigram_overlap  \\\n",
       "count          7165.000000          7165.000000              7165.000000   \n",
       "mean             34.973761             0.256570                 0.045554   \n",
       "std              25.553950             0.245381                 0.064807   \n",
       "min               6.000000             0.000000                 0.000000   \n",
       "25%              18.000000             0.062500                 0.000000   \n",
       "50%              27.000000             0.177419                 0.024390   \n",
       "75%              43.000000             0.391753                 0.066667   \n",
       "max             325.000000             1.000000                 0.600000   \n",
       "\n",
       "       text_bigram_ratio  text_bigram_diff  question_bigram_diff  \\\n",
       "count        7165.000000       7165.000000           7165.000000   \n",
       "mean            0.110370          0.743430              0.954446   \n",
       "std             0.081772          0.245381              0.064807   \n",
       "min             0.016588          0.000000              0.400000   \n",
       "25%             0.058419          0.608247              0.933333   \n",
       "50%             0.085911          0.822581              0.975610   \n",
       "75%             0.134021          0.937500              1.000000   \n",
       "max             1.093284          1.000000              1.000000   \n",
       "\n",
       "       text_bigram_exclusive  question_bigram_exclusive  \n",
       "count            7165.000000                7165.000000  \n",
       "mean                1.895557                   0.234856  \n",
       "std                 0.130174                   0.114135  \n",
       "min                 0.424000                   0.039867  \n",
       "25%                 1.855626                   0.155477  \n",
       "50%                 1.944637                   0.206186  \n",
       "75%                 1.985765                   0.281437  \n",
       "max                 2.000000                   1.088000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.select_dtypes(include=np.number).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(f_cols):\n",
    "    \n",
    "    model_params = {\n",
    "        'objective': 'fair', \n",
    "        'verbose': 0, \n",
    "        'force_col_wise': True,\n",
    "        'learning_rate': 0.08,\n",
    "        'boosting_type': 'dart',\n",
    "        'num_leaves': 11,\n",
    "    }\n",
    "\n",
    "    metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nMCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.806634  0.458984  0.357693  train  content\n",
      "mean  0.760823  0.512585  0.401044    val  content\n",
      "mean  0.661757  0.600901  0.473157  train  wording\n",
      "mean  0.438680  0.758031  0.606133    val  wording\n",
      "std   0.004038  0.015497  0.011840  train  content\n",
      "std   0.023556  0.057368  0.040232    val  content\n",
      "std   0.029967  0.020707  0.015021  train  wording\n",
      "std   0.066966  0.093496  0.079438    val  wording\n",
      "\n",
      "MCRMSE: 0.635307968278987\n",
      "\n",
      "    importance                         feature\n",
      "3          270             text_unique_bigrams\n",
      "4          252             text_bigram_overlap\n",
      "6          135               text_bigram_ratio\n",
      "7          117                text_bigram_diff\n",
      "9           85           text_bigram_exclusive\n",
      "10          67       question_bigram_exclusive\n",
      "0           44     prompt_title_unique_bigrams\n",
      "5           23         question_bigram_overlap\n",
      "8            5            question_bigram_diff\n",
      "2            2      prompt_text_unique_bigrams\n",
      "1            0  prompt_question_unique_bigrams\n"
     ]
    }
   ],
   "source": [
    "train(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.790097  0.478199  0.371981  train  content\n",
      "mean  0.758234  0.515405  0.401389    val  content\n",
      "mean  0.616357  0.640318  0.503853  train  wording\n",
      "mean  0.510684  0.705167  0.559819    val  wording\n",
      "std   0.002559  0.014749  0.010732  train  content\n",
      "std   0.017286  0.053294  0.036332    val  content\n",
      "std   0.033239  0.031048  0.021957  train  wording\n",
      "std   0.128460  0.131434  0.109709    val  wording\n",
      "\n",
      "MCRMSE: 0.6102856988744446\n",
      "\n",
      "   importance              feature\n",
      "0         527  text_bigram_overlap\n",
      "1         473  text_unique_bigrams\n"
     ]
    }
   ],
   "source": [
    "train(['text_bigram_overlap', 'text_unique_bigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.792974  0.474924  0.369432  train  content\n",
      "mean  0.759408  0.514202  0.400492    val  content\n",
      "mean  0.625913  0.632181  0.496424  train  wording\n",
      "mean  0.502043  0.711969  0.564591    val  wording\n",
      "std   0.003546  0.015729  0.011344  train  content\n",
      "std   0.019022  0.055204  0.037566    val  content\n",
      "std   0.033135  0.029229  0.019858  train  wording\n",
      "std   0.121501  0.127058  0.106459    val  wording\n",
      "\n",
      "MCRMSE: 0.613085594970603\n",
      "\n",
      "   importance                feature\n",
      "1         481    text_bigram_overlap\n",
      "2         442    text_unique_bigrams\n",
      "0          77  text_bigram_exclusive\n"
     ]
    }
   ],
   "source": [
    "train(['text_bigram_exclusive', 'text_bigram_overlap', 'text_unique_bigrams'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, it looks like adding more n-gram based features does improve train-set performance.  However due to low variance in the train set, any features that encode information based on the prompt question, title or text lead to heavy overfitting, so the best validation performance so far has been seen using only the n_unique_bigrams and the size of the intersection of bigrams in the summary and original text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
