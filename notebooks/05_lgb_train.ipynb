{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/chris/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "data_dir = Path('../data')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='inner', on='prompt_id')\n",
    "\n",
    "    if len(df) != len(summaries_df):\n",
    "        raise AssertionError('Could not match all prompt ids to a prompt')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "\n",
    "def nlp_splits(df: pd.DataFrame, column: str) -> None:\n",
    "    output = Parallel(n_jobs=4)(delayed(clear_stopwords)(df[column], idx) for idx in range(len(df)))\n",
    "\n",
    "    df[f'{column}_tokens'] = [part[0] for part in output]\n",
    "    df[f'{column}_no_stopwords'] = [part[1] for part in output]\n",
    "    df[f'{column}_lemmas'] = [part[2] for part in output]\n",
    "    df[f'{column}_bigram'] = [part[3] for part in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = make_split(summaries_train, prompts_train)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in text_columns:\n",
    "    nlp_splits(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()\n",
    "\n",
    "\n",
    "df_train = df.copy(deep=True)\n",
    "\n",
    "# Create n-gram based features\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_lgb_kfold(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst\n",
    "\n",
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    \n",
    "    X_train = features[feature_names].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "    y_train = targets[target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "    train_errors = [calculate_errors(y_train, bst.predict(X_train))]\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors)\n",
    "\n",
    "    return train_metrics, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(f_cols, model_params):\n",
    "    metric_df_content, bst_content = train_lgb_kfold('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb_kfold('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nMCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.790097  0.478199  0.371981  train  content\n",
      "mean  0.758234  0.515405  0.401389    val  content\n",
      "mean  0.616357  0.640318  0.503853  train  wording\n",
      "mean  0.510684  0.705167  0.559819    val  wording\n",
      "std   0.002559  0.014749  0.010732  train  content\n",
      "std   0.017286  0.053294  0.036332    val  content\n",
      "std   0.033239  0.031048  0.021957  train  wording\n",
      "std   0.128460  0.131434  0.109709    val  wording\n",
      "\n",
      "MCRMSE: 0.6102856988744446\n",
      "\n",
      "   importance              feature\n",
      "0         527  text_bigram_overlap\n",
      "1         473  text_unique_bigrams\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'objective': 'fair', \n",
    "    'verbose': 0, \n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.08,\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 11,\n",
    "}\n",
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "\tContent scores\n",
      "         r2      rmse      mae\n",
      "0  0.785977  0.482748  0.37519\n",
      "\n",
      "-----------------------------------\n",
      "\tWording scores\n",
      "         r2      rmse       mae\n",
      "0  0.611153  0.646009  0.508004\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{\"-\"*35}\\n\\tContent scores')\n",
    "pprint(metric_df_content)\n",
    "print(f'\\n{\"-\"*35}\\n\\tWording scores')\n",
    "pprint(metric_df_wording)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
