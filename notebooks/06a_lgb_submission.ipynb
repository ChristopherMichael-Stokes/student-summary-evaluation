{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-22T10:07:54.596039Z","iopub.status.busy":"2023-08-22T10:07:54.595656Z","iopub.status.idle":"2023-08-22T10:07:59.266913Z","shell.execute_reply":"2023-08-22T10:07:59.265599Z","shell.execute_reply.started":"2023-08-22T10:07:54.596015Z"},"trusted":true},"outputs":[],"source":["import os\n","download_dir = '/kaggle/input/commonlit-lgb-nlp-train-01/nltk_data'\n","os.environ['NLTK_DATA'] = download_dir\n","\n","import nltk\n","nltk.data.path.append(download_dir)\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk import ngrams\n","\n","import datasets\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from joblib import Parallel, delayed\n","import plotly.express as px\n","from sklearn.model_selection import GroupKFold\n","from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n","import lightgbm as lgb\n","from math import sqrt\n","\n","from collections import defaultdict\n","from functools import partial, reduce\n","from operator import or_\n","from pathlib import Path\n","from pprint import pprint\n","from typing import Optional, Union, List, Tuple, Dict, Set, Any"]},{"cell_type":"raw","metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:07:59.269071Z","iopub.status.busy":"2023-08-22T10:07:59.268730Z","iopub.status.idle":"2023-08-22T10:07:59.298050Z","shell.execute_reply":"2023-08-22T10:07:59.296880Z","shell.execute_reply.started":"2023-08-22T10:07:59.269040Z"},"trusted":true},"source":["%%sh\n","ls /kaggle/input"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:07:59.299927Z","iopub.status.busy":"2023-08-22T10:07:59.299584Z","iopub.status.idle":"2023-08-22T10:07:59.324346Z","shell.execute_reply":"2023-08-22T10:07:59.323072Z","shell.execute_reply.started":"2023-08-22T10:07:59.299903Z"},"trusted":true},"outputs":[],"source":["stop_words = set(stopwords.words('english'))\n","lemmatiser = WordNetLemmatizer()\n","\n","kaggle_input = Path('/kaggle/input')\n","# train_output = Path('commonlit-lgb-nlp-train-01')\n","train_output = Path('commonlit-lgb-lucky')\n","data_dir = kaggle_input / 'commonlit-evaluate-student-summaries'\n","\n","sample_submission = data_dir / 'sample_submission.csv'\n","summaries_train = data_dir / 'summaries_train.csv'\n","summaries_test = data_dir / 'summaries_test.csv'\n","prompts_train = data_dir / 'prompts_train.csv'\n","prompts_test = data_dir / 'prompts_test.csv'\n","\n","content_model = kaggle_input / train_output / 'content.txt'\n","wording_model = kaggle_input / train_output / 'wording.txt'\n","word_dictionary = kaggle_input / train_output / 'commonlit_words.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:07:59.327428Z","iopub.status.busy":"2023-08-22T10:07:59.326421Z","iopub.status.idle":"2023-08-22T10:07:59.332440Z","shell.execute_reply":"2023-08-22T10:07:59.331314Z","shell.execute_reply.started":"2023-08-22T10:07:59.327401Z"},"trusted":true},"outputs":[],"source":["def predict(model: lgb.Booster, df: pd.DataFrame, features: List[str]) -> pd.Series:\n","    return model.predict(df[features])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:07:59.334477Z","iopub.status.busy":"2023-08-22T10:07:59.333584Z","iopub.status.idle":"2023-08-22T10:07:59.348699Z","shell.execute_reply":"2023-08-22T10:07:59.347478Z","shell.execute_reply.started":"2023-08-22T10:07:59.334452Z"},"trusted":true},"outputs":[],"source":["def text_tokenize(text: str) -> List[str]:\n","    return [lemmatiser.lemmatize(tok.lower()) for tok in word_tokenize(text) if tok.isalnum() and tok not in stop_words]\n","\n","        \n","def make_bigram(tokens: List[str]) -> Set[str]:\n","    if type(tokens) != list:\n","        tokens = tokens.tolist()\n","    return set(ngrams(tokens, 2))\n","\n","\n","def tokenize(row: Dict[str, Any]) -> List[str]:\n","    for col in ['prompt_text', 'prompt_title', 'prompt_question', 'prompt_text']:\n","        row[f'{col}_lemmas'] = lemmas = text_tokenize(row[col])\n","        row[f'{col}_bigram'] = make_bigram(lemmas)\n","\n","        \n","def nlp_preprocess(df: pd.DataFrame, column: str):\n","    df[f'{column}_lemmas'] = df[column].apply(text_tokenize)\n","    df[f'{column}_bigram'] = df[f'{column}_lemmas'].apply(make_bigram)\n","    \n","    \n","def batch_tokenize(data: Dict[str, Any]) -> List[str]:\n","    lemmas = [text_tokenize(row) for row in data]\n","    return lemmas\n","\n","def process_col(data: Dict[str, Any], col: str) -> List[str]:\n","    lemmas = [text_tokenize(row) for row in data]\n","    bigrams = [make_bigram(lemma) for lemma in lemmas]\n","    return {f'{col}_lemmas': lemmas, f'{col}_bigram': bigrams}\n","\n","def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n","    summaries_df = pd.read_csv(summaries_path)\n","    prompts_df = pd.read_csv(prompts_path)\n","    \n","    for column in ['prompt_title', 'prompt_question', 'prompt_text']:\n","        nlp_preprocess(prompts_df, column)\n","        prompts_df[f'{column}_unique_bigrams'] = prompts_df[f'{column}_bigram'].str.len()\n","    \n","    summaries_dataset = datasets.Dataset.from_pandas(summaries_df, preserve_index=False)\n","    proc_func = partial(process_col, col='text')\n","    summaries_df = summaries_dataset.map(function=lambda example: {**proc_func(example['text']), **example}, num_proc=os.cpu_count(), keep_in_memory=True, batched=True).to_pandas()\n","    summaries_df['text_bigram'] = summaries_df.text_bigram.apply(lambda row: {(x[0], x[1]) for x in row})\n","    summaries_df['text_unique_bigrams'] = summaries_df['text_bigram'].str.len()\n","    \n","    df = pd.merge(summaries_df, prompts_df, how='left', on='prompt_id')\n","    df.fillna('')\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:07:59.350506Z","iopub.status.busy":"2023-08-22T10:07:59.350151Z","iopub.status.idle":"2023-08-22T10:08:01.419201Z","shell.execute_reply":"2023-08-22T10:08:01.418241Z","shell.execute_reply.started":"2023-08-22T10:07:59.350474Z"},"trusted":true},"outputs":[],"source":["# Load data\n","# df = make_split(summaries_train, prompts_train)\n","df = make_split(summaries_test, prompts_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.422032Z","iopub.status.busy":"2023-08-22T10:08:01.420755Z","iopub.status.idle":"2023-08-22T10:08:01.443446Z","shell.execute_reply":"2023-08-22T10:08:01.442318Z","shell.execute_reply.started":"2023-08-22T10:08:01.421990Z"},"trusted":true},"outputs":[],"source":["df_test = df\n","\n","# Create n-gram based features\n","df_test['text_bigram_overlap'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_test.text_unique_bigrams\n","df_test['question_bigram_overlap'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_test.text_unique_bigrams\n","df_test['text_bigram_ratio'] = df_test['text_unique_bigrams'] / (df_test['prompt_text_unique_bigrams'])\n","\n","df_test['text_bigram_diff'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_test.text_unique_bigrams\n","df_test['question_bigram_diff'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_test.text_unique_bigrams\n","\n","df_test['text_bigram_exclusive'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_test.text_unique_bigrams\n","df_test['question_bigram_exclusive'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_test.text_unique_bigrams"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.445006Z","iopub.status.busy":"2023-08-22T10:08:01.444618Z","iopub.status.idle":"2023-08-22T10:08:01.464771Z","shell.execute_reply":"2023-08-22T10:08:01.463099Z","shell.execute_reply.started":"2023-08-22T10:08:01.444973Z"},"trusted":true},"outputs":[],"source":["# Create word based features\n","df_test['n_words'] = df_test.text_lemmas.str.len()\n","df_test['unique_words'] = df_test.text_lemmas.apply(set).str.len()\n","df_test['unique_ratio'] = df_test.unique_words / df_test.n_words"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.466909Z","iopub.status.busy":"2023-08-22T10:08:01.466556Z","iopub.status.idle":"2023-08-22T10:08:01.484532Z","shell.execute_reply":"2023-08-22T10:08:01.483115Z","shell.execute_reply.started":"2023-08-22T10:08:01.466883Z"},"trusted":true},"outputs":[],"source":["df_test['word_lengths'] = df_test.text_lemmas.apply(lambda x: [len(y) for y in x])\n","df_test['word_len_avg'] = df_test.word_lengths.apply(np.mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.489385Z","iopub.status.busy":"2023-08-22T10:08:01.488099Z","iopub.status.idle":"2023-08-22T10:08:01.503554Z","shell.execute_reply":"2023-08-22T10:08:01.502367Z","shell.execute_reply.started":"2023-08-22T10:08:01.489327Z"},"trusted":true},"outputs":[],"source":["df_test['word_len_q10'] = df_test.word_lengths.apply(partial(np.percentile, q=10))\n","df_test['word_len_q90'] = df_test.word_lengths.apply(partial(np.percentile, q=90))"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-08-21T22:01:28.306057Z","iopub.status.busy":"2023-08-21T22:01:28.305005Z","iopub.status.idle":"2023-08-21T22:01:28.681545Z","shell.execute_reply":"2023-08-21T22:01:28.680374Z","shell.execute_reply.started":"2023-08-21T22:01:28.306024Z"}},"source":["with open(word_dictionary, 'r') as f:\n","    word_set = set([line.strip() for line in f.read().split('\\n')])\n","    \n","def get_unique_words(col: str) -> Set[str]:\n","    word_sets = df_test[col].apply(set).tolist()\n","    return reduce(or_, word_sets)\n","\n","prompt_words = get_unique_words('prompt_text_lemmas')\n","question_words = get_unique_words('prompt_question_lemmas')\n","title_words = get_unique_words('prompt_title_lemmas')\n","\n","word_set = word_set | prompt_words | question_words | title_words"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-08-21T22:01:28.685061Z","iopub.status.busy":"2023-08-21T22:01:28.684339Z","iopub.status.idle":"2023-08-21T22:01:28.692254Z","shell.execute_reply":"2023-08-21T22:01:28.691090Z","shell.execute_reply.started":"2023-08-21T22:01:28.685017Z"}},"source":["count_missing_words = lambda tokens: sum([word not in word_set for word in tokens if word.isalpha()])\n","df_test['missing_wordcount'] = df_test.text_lemmas.apply(count_missing_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.506175Z","iopub.status.busy":"2023-08-22T10:08:01.504975Z","iopub.status.idle":"2023-08-22T10:08:01.580217Z","shell.execute_reply":"2023-08-22T10:08:01.578762Z","shell.execute_reply.started":"2023-08-22T10:08:01.506128Z"},"trusted":true},"outputs":[],"source":["content_bst = lgb.Booster(model_file=content_model)\n","wording_bst = lgb.Booster(model_file=wording_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-22T10:08:01.582815Z","iopub.status.busy":"2023-08-22T10:08:01.582039Z","iopub.status.idle":"2023-08-22T10:08:01.604341Z","shell.execute_reply":"2023-08-22T10:08:01.603493Z","shell.execute_reply.started":"2023-08-22T10:08:01.582776Z"},"trusted":true},"outputs":[],"source":["features = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n","          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90']\n","           #'missing_wordcount']\n","df['content'] = predict(content_bst, df_test, features)\n","df['wording'] = predict(wording_bst, df_test, features)\n","\n","submission_df = df[['student_id', 'content', 'wording']]\n","submission_df.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
