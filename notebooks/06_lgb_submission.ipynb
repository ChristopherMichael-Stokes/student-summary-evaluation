{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/chris/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "data_directory = '../data'\n",
    "prompt_inputs = 'prompts_test.csv'\n",
    "summaries_input = 'summaries_test.csv'\n",
    "submission_csv = 'submission.csv'\n",
    "\n",
    "content_model_file = 'models/content_lgb_model.txt'\n",
    "wording_model_file = 'models/wording_lgb_model.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(data_directory).absolute()\n",
    "\n",
    "prompt_input_path = data_path / prompt_inputs\n",
    "summaries_input_path = data_path / summaries_input\n",
    "submission_csv_path = data_path / submission_csv\n",
    "content_model_path = data_path / content_model_file\n",
    "wording_model_path = data_path / wording_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='inner', on='prompt_id')\n",
    "\n",
    "    if len(df) != len(summaries_df):\n",
    "        raise AssertionError('Could not match all prompt ids to a prompt')\n",
    "    \n",
    "    return df\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "\n",
    "def nlp_splits(df: pd.DataFrame, column: str) -> None:\n",
    "    output = Parallel(n_jobs=4)(delayed(clear_stopwords)(df[column], idx) for idx in range(len(df)))\n",
    "\n",
    "    df[f'{column}_tokens'] = [part[0] for part in output]\n",
    "    df[f'{column}_no_stopwords'] = [part[1] for part in output]\n",
    "    df[f'{column}_lemmas'] = [part[2] for part in output]\n",
    "    df[f'{column}_bigram'] = [part[3] for part in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text: str) -> Set[str]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(text) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_split(summaries_input_path, prompt_input_path)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in text_columns:\n",
    "    df[f'{column}_bigram'] = df[column].apply(get_bigrams) # TODO: parallelise\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()\n",
    "\n",
    "df['text_bigram_overlap'] = df[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_lgb_model = lgb.Booster(model_file=content_model_path)\n",
    "wording_lgb_model = lgb.Booster(model_file=wording_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: lgb.Booster, df: pd.DataFrame, features: List[str]) -> pd.Series:\n",
    "    return model.predict(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['text_bigram_overlap', 'text_unique_bigrams']\n",
    "df['content'] = predict(content_lgb_model, df, features)\n",
    "df['wording'] = predict(wording_lgb_model, df, features)\n",
    "\n",
    "submission_df = df[['student_id', 'content', 'wording']]\n",
    "submission_df.to_csv(submission_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
