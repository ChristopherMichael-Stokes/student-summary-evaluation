{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom collections import defaultdict\nfrom functools import partial, reduce\nfrom math import sqrt\nfrom operator import or_\nfrom pathlib import Path\nfrom pprint import pprint\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\nimport datasets\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor, Pool\nfrom catboost.utils import eval_metric\nfrom nltk import ngrams\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom tqdm import tqdm\n\ndownload_dir = f'/kaggle/input/commonlit-cat-nlp-train-01/nltk_data'\nos.environ['NLTK_DATA'] = download_dir\n\nnltk.data.path.append(download_dir)\n\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:36.054810Z","iopub.execute_input":"2023-08-29T19:16:36.055194Z","iopub.status.idle":"2023-08-29T19:16:40.482664Z","shell.execute_reply.started":"2023-08-29T19:16:36.055161Z","shell.execute_reply":"2023-08-29T19:16:40.481148Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatiser = WordNetLemmatizer()\n\nkaggle_input = Path('/kaggle/input')\n# train_output = Path('commonlit-lgb-nlp-train-01')\n# train_output = Path('commonlit-lgb-lucky')\ntrain_output = Path('commonlit-cat-nlp-train-01')\ndata_dir = kaggle_input / 'commonlit-evaluate-student-summaries'\n\nsample_submission = data_dir / 'sample_submission.csv'\nsummaries_train = data_dir / 'summaries_train.csv'\nsummaries_test = data_dir / 'summaries_test.csv'\nprompts_train = data_dir / 'prompts_train.csv'\nprompts_test = data_dir / 'prompts_test.csv'\n\n\ncontent_model = kaggle_input / train_output / 'content.txt'\nwording_model = kaggle_input / train_output / 'wording.txt'\nword_dictionary = kaggle_input / train_output / 'commonlit_words.txt'","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:40.485110Z","iopub.execute_input":"2023-08-29T19:16:40.486008Z","iopub.status.idle":"2023-08-29T19:16:40.510114Z","shell.execute_reply.started":"2023-08-29T19:16:40.485964Z","shell.execute_reply":"2023-08-29T19:16:40.508762Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def text_tokenize(text: str) -> List[str]:\n    return [lemmatiser.lemmatize(tok.lower()) for tok in word_tokenize(text) if tok.isalnum() and tok not in stop_words]\n\n\ndef make_bigram(tokens: List[str]) -> Set[str]:\n    if type(tokens) != list:\n        tokens = tokens.tolist()\n    return set(ngrams(tokens, 2))\n\n\ndef tokenize(row: Dict[str, Any]) -> List[str]:\n    for col in ['prompt_text', 'prompt_title', 'prompt_question', 'prompt_text']:\n        row[f'{col}_lemmas'] = lemmas = text_tokenize(row[col])\n        row[f'{col}_bigram'] = make_bigram(lemmas)\n\n\ndef nlp_preprocess(df: pd.DataFrame, column: str):\n    df[f'{column}_lemmas'] = df[column].apply(text_tokenize)\n    df[f'{column}_bigram'] = df[f'{column}_lemmas'].apply(make_bigram)\n\n\n# def batch_tokenize(data: Dict[str, Any]) -> List[str]:\n#     lemmas = [text_tokenize(row) for row in data]\n#     return lemmas\n\n\ndef process_col(data: Dict[str, Any], col: str) -> List[str]:\n    lemmas = [text_tokenize(text) for text in data]\n    bigrams = [make_bigram(lemma) for lemma in lemmas]\n    n_stopwords = []\n    for row in lemmas:\n        n_stopwords.append(sum([lemma in stop_words for lemma in row]))\n#     pos = [pos_tag(lemma, tagset='universal') for lemma in lemmas]\n    sentences = [sent_tokenize(text) for text in data]\n    sentence_len_avg, sentence_len_std = [], []\n    for i in range(len(sentences)):\n        for j in range(len(sentences[i])):\n            sentences[i][j] = len(sentences[i][j].split())\n        sentence_len_avg.append(np.mean(sentences[i]))\n        sentence_len_std.append(np.std(sentences[i]))\n\n    return {f'{col}_lemmas': lemmas,\n            f'{col}_bigram': bigrams,\n            f'{col}_sentence_len_avg': sentence_len_avg,\n            f'{col}_sentence_len_std': sentence_len_std,\n            f'{col}_stopwords': n_stopwords,\n#             'pos': pos\n            }\n\n\ndef make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n    summaries_df = pd.read_csv(summaries_path)\n    prompts_df = pd.read_csv(prompts_path)\n\n    for column in ['prompt_title', 'prompt_question', 'prompt_text']:\n        nlp_preprocess(prompts_df, column)\n        prompts_df[f'{column}_unique_bigrams'] = prompts_df[f'{column}_bigram'].str.len()\n\n    summaries_dataset = datasets.Dataset.from_pandas(summaries_df, preserve_index=False)\n    proc_func = partial(process_col, col='text')\n    summaries_df = summaries_dataset.map(function=lambda example: {\n                                         **proc_func(example['text']), **example}, num_proc=os.cpu_count(), keep_in_memory=True, batched=True).to_pandas()\n    summaries_df['text_bigram'] = summaries_df.text_bigram.apply(lambda row: {(x[0], x[1]) for x in row})\n    summaries_df['text_unique_bigrams'] = summaries_df['text_bigram'].str.len()\n\n    df = pd.merge(summaries_df, prompts_df, how='left', on='prompt_id')\n    df.fillna('')\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:40.512641Z","iopub.execute_input":"2023-08-29T19:16:40.513214Z","iopub.status.idle":"2023-08-29T19:16:40.822028Z","shell.execute_reply.started":"2023-08-29T19:16:40.513176Z","shell.execute_reply":"2023-08-29T19:16:40.819884Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf = make_split(summaries_test, prompts_test)\n# df = make_split(summaries_train, prompts_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:40.825884Z","iopub.execute_input":"2023-08-29T19:16:40.826792Z","iopub.status.idle":"2023-08-29T19:16:44.150012Z","shell.execute_reply.started":"2023-08-29T19:16:40.826737Z","shell.execute_reply":"2023-08-29T19:16:44.148804Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca01a3ffd6c4a299b366b0998c318de"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"accae9f8540e42c69044e828b1edd76a"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc1d30a9dd744caa0e770f27ad79e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62fba4fcc5f44743ad52e36b42be99c4"}},"metadata":{}}]},{"cell_type":"code","source":"df_test = df#.copy(deep=True)\n\n# Create n-gram based features\ndf_test['text_bigram_overlap'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(\n    lambda row: len(row[0] & row[1]), axis=1) / df_test.text_unique_bigrams\ndf_test['question_bigram_overlap'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(\n    lambda row: len(row[0] & row[1]), axis=1) / df_test.text_unique_bigrams\ndf_test['text_bigram_ratio'] = df_test['text_unique_bigrams'] / (df_test['prompt_text_unique_bigrams'])\n\ndf_test['text_bigram_diff'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(\n    lambda row: len(row[1] - row[0]), axis=1) / df_test.text_unique_bigrams\ndf_test['question_bigram_diff'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(\n    lambda row: len(row[1] - row[0]), axis=1) / df_test.text_unique_bigrams\n\ndf_test['text_bigram_exclusive'] = df_test[['prompt_text_bigram', 'text_bigram']].apply(\n    lambda row: len(row[0] ^ row[1]), axis=1) / df_test.text_unique_bigrams\ndf_test['question_bigram_exclusive'] = df_test[['prompt_question_bigram', 'text_bigram']].apply(\n    lambda row: len(row[0] ^ row[1]), axis=1) / df_test.text_unique_bigrams","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.153454Z","iopub.execute_input":"2023-08-29T19:16:44.154026Z","iopub.status.idle":"2023-08-29T19:16:44.186638Z","shell.execute_reply.started":"2023-08-29T19:16:44.153971Z","shell.execute_reply":"2023-08-29T19:16:44.184714Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_test['n_words'] = df_test.text_lemmas.str.len()\ndf_test['unique_words'] = df_test.text_lemmas.apply(set).str.len()\ndf_test['unique_ratio'] = df_test.unique_words / df_test.n_words","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.190318Z","iopub.execute_input":"2023-08-29T19:16:44.191069Z","iopub.status.idle":"2023-08-29T19:16:44.210451Z","shell.execute_reply.started":"2023-08-29T19:16:44.191024Z","shell.execute_reply":"2023-08-29T19:16:44.209367Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_test['word_lengths'] = df_test.text_lemmas.apply(lambda x: [len(y) for y in x])\ndf_test['word_len_avg'] = df_test.word_lengths.apply(np.mean)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.212156Z","iopub.execute_input":"2023-08-29T19:16:44.212722Z","iopub.status.idle":"2023-08-29T19:16:44.225030Z","shell.execute_reply.started":"2023-08-29T19:16:44.212670Z","shell.execute_reply":"2023-08-29T19:16:44.223579Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_test['word_len_q10'] = df_test.word_lengths.apply(partial(np.percentile, q=10))\ndf_test['word_len_q90'] = df_test.word_lengths.apply(partial(np.percentile, q=90))\ndf_test['word_len_std'] = df_test.word_lengths.apply(np.std)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.226774Z","iopub.execute_input":"2023-08-29T19:16:44.227472Z","iopub.status.idle":"2023-08-29T19:16:44.249670Z","shell.execute_reply.started":"2023-08-29T19:16:44.227437Z","shell.execute_reply":"2023-08-29T19:16:44.248430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"def pos_counts(tags):\n    dd = defaultdict(lambda: 0)\n    for _, pos in tags:\n        dd[pos] += 1\n    return dd\n\n\ndf_test['pos_counts'] = df_test.pos.apply(pos_counts)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T13:08:12.346227Z","iopub.execute_input":"2023-08-28T13:08:12.347055Z","iopub.status.idle":"2023-08-28T13:08:12.354191Z","shell.execute_reply.started":"2023-08-28T13:08:12.347026Z","shell.execute_reply":"2023-08-28T13:08:12.353388Z"}}},{"cell_type":"markdown","source":"df_test['verb_count'] = df_test.pos_counts.str['VERB'].replace(np.nan, 0)\ndf_test['noun_count'] = df_test.pos_counts.str['NOUN'].replace(np.nan, 0)\ndf_test['adv_count'] = df_test.pos_counts.str['ADV'].replace(np.nan, 0)\ndf_test['adj_count'] = df_test.pos_counts.str['ADJ'].replace(np.nan, 0)\ndf_test['det_count'] = df_test.pos_counts.str['DET'].replace(np.nan, 0)","metadata":{"execution":{"iopub.status.busy":"2023-08-28T13:08:12.357077Z","iopub.execute_input":"2023-08-28T13:08:12.358006Z","iopub.status.idle":"2023-08-28T13:08:12.372130Z","shell.execute_reply.started":"2023-08-28T13:08:12.357975Z","shell.execute_reply":"2023-08-28T13:08:12.370955Z"}}},{"cell_type":"code","source":"with open(word_dictionary, 'r') as f:\n    en_words = set(word.strip() for word in f.read().split())","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.251166Z","iopub.execute_input":"2023-08-29T19:16:44.252065Z","iopub.status.idle":"2023-08-29T19:16:44.582968Z","shell.execute_reply.started":"2023-08-29T19:16:44.252015Z","shell.execute_reply":"2023-08-29T19:16:44.581628Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_unique_words(col: str) -> Set[str]:\n    word_sets = df_test[col].apply(set).tolist()\n    return reduce(or_, word_sets)\n\n\nprompt_words = get_unique_words('prompt_text_lemmas')\nquestion_words = get_unique_words('prompt_question_lemmas')\ntitle_words = get_unique_words('prompt_title_lemmas')\n\nword_set = en_words | prompt_words | question_words | title_words","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.588541Z","iopub.execute_input":"2023-08-29T19:16:44.588934Z","iopub.status.idle":"2023-08-29T19:16:44.704621Z","shell.execute_reply.started":"2023-08-29T19:16:44.588903Z","shell.execute_reply":"2023-08-29T19:16:44.702966Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def count_missing_words(tokens): return sum([word not in word_set for word in tokens if word.isalpha()])\n\n\ndf_test['missing_wordcount'] = df_test.text_lemmas.apply(count_missing_words)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.706555Z","iopub.execute_input":"2023-08-29T19:16:44.706963Z","iopub.status.idle":"2023-08-29T19:16:44.723885Z","shell.execute_reply.started":"2023-08-29T19:16:44.706929Z","shell.execute_reply":"2023-08-29T19:16:44.722516Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"numeric_features = df_test.select_dtypes(include=np.number)\ntarget_columns = ['content', 'wording']\nfeature_columns = [col for col in numeric_features if col not in target_columns]\n\nfeatures = numeric_features[feature_columns]\nprompt_group = pd.Categorical(df['prompt_title'])","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.725860Z","iopub.execute_input":"2023-08-29T19:16:44.726379Z","iopub.status.idle":"2023-08-29T19:16:44.741936Z","shell.execute_reply.started":"2023-08-29T19:16:44.726343Z","shell.execute_reply":"2023-08-29T19:16:44.740823Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"f_cols_content = ['text_unique_bigrams',\n                  'text_bigram_overlap',\n                  'text_bigram_ratio',\n                  'text_bigram_diff',\n                  'text_bigram_exclusive',\n                  'question_bigram_exclusive',\n                  'n_words',\n                  'unique_words',\n                  'unique_ratio',\n                  'verb_count',\n                  'noun_count',\n                  'adv_count',\n                  'adj_count',\n                  'det_count',\n                  'missing_wordcount']\n\nf_cols_wording = ['text_sentence_len_std',\n                  'text_unique_bigrams',\n                  'text_bigram_ratio',\n                  'text_bigram_exclusive',\n                  'question_bigram_exclusive',\n                  'n_words',\n                  'unique_words',\n                  'unique_ratio',\n                  'verb_count',\n                  'noun_count',\n                  'adv_count',\n                  'adj_count',\n                  'det_count',\n                  'missing_wordcount']","metadata":{"execution":{"iopub.status.busy":"2023-08-28T12:36:50.973284Z","iopub.execute_input":"2023-08-28T12:36:50.973750Z","iopub.status.idle":"2023-08-28T12:36:50.982219Z","shell.execute_reply.started":"2023-08-28T12:36:50.973708Z","shell.execute_reply":"2023-08-28T12:36:50.981133Z"}}},{"cell_type":"code","source":"f_cols_content = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n          'n_words', 'unique_words', 'word_len_avg', 'word_len_std',\n         'missing_wordcount', 'text_sentence_len_std']\nf_cols_wording = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n          'n_words', 'unique_words', 'word_len_avg', 'word_len_std',\n         'missing_wordcount', 'text_sentence_len_std']","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.743535Z","iopub.execute_input":"2023-08-29T19:16:44.744017Z","iopub.status.idle":"2023-08-29T19:16:44.813452Z","shell.execute_reply.started":"2023-08-29T19:16:44.743971Z","shell.execute_reply":"2023-08-29T19:16:44.811968Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"content_bst = CatBoostRegressor().load_model(fname=content_model)\nwording_bst = CatBoostRegressor().load_model(fname=wording_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.815374Z","iopub.execute_input":"2023-08-29T19:16:44.815820Z","iopub.status.idle":"2023-08-29T19:16:44.884117Z","shell.execute_reply.started":"2023-08-29T19:16:44.815776Z","shell.execute_reply":"2023-08-29T19:16:44.882731Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df['content'] = content_bst.predict(df_test[f_cols_content])\ndf['wording'] = wording_bst.predict(df_test[f_cols_wording])\n\nsubmission_df = df[['student_id', 'content', 'wording']]\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T19:16:44.885782Z","iopub.execute_input":"2023-08-29T19:16:44.887020Z","iopub.status.idle":"2023-08-29T19:16:44.965451Z","shell.execute_reply.started":"2023-08-29T19:16:44.886972Z","shell.execute_reply":"2023-08-29T19:16:44.964330Z"},"trusted":true},"execution_count":15,"outputs":[]}]}