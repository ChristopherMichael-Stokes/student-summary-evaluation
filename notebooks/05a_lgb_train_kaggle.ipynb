{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and nltk downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chris/repos/student-\n",
      "[nltk_data]     summary-evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chris/repos/student-\n",
      "[nltk_data]     summary-evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/chris/repos/student-summary-\n",
      "[nltk_data]     evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/chris/repos/student-\n",
      "[nltk_data]     summary-evaluation/notebooks/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "download_dir = f'{os.getcwd()}/nltk_data'\n",
    "os.environ['NLTK_DATA'] = download_dir\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir=download_dir)\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('wordnet', download_dir=download_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=download_dir)\n",
    "nltk.download('universal_tagset', download_dir=download_dir)\n",
    "nltk.download('words', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ngrams\n",
    "import spellwise\n",
    "from spellwise import Levenshtein\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from functools import partial, reduce\n",
    "from operator import or_\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Fix nltk installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mbrown\u001b[m\u001b[m\n",
      "brown.zip\n",
      "\u001b[1m\u001b[36mstopwords\u001b[m\u001b[m\n",
      "stopwords.zip\n",
      "\u001b[1m\u001b[36mwordnet\u001b[m\u001b[m\n",
      "wordnet.zip\n",
      "\u001b[1m\u001b[36mwords\u001b[m\u001b[m\n",
      "words.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replace nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/data.verb? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/index.adv? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/adv.exc? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/index.verb? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/cntlist.rev? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/data.adj? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/index.adj? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/citation.bib? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/noun.exc? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/verb.exc? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/index.sense? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/data.noun? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/data.adv? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/index.noun? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace nltk_data/corpora/wordnet/adj.exc? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "yes | unzip -q nltk_data/corpora/wordnet.zip -d nltk_data/corpora/\n",
    "ls nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "tqdm.pandas()\n",
    "\n",
    "data_dir = Path('../data/commonlit-evaluate-student-summaries')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'\n",
    "\n",
    "content_model = '../data/models/content.txt'\n",
    "wording_model = '../data/models/wording.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path)#, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path)#, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='left', on='prompt_id')\n",
    "    df.fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return [lemmatiser.lemmatize(tok.lower()) for tok in word_tokenize(text) if tok.isalnum() and tok not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigram(tokens: List[str]) -> Set[str]:\n",
    "    return set(ngrams(tokens, 2))\n",
    "\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "    \n",
    "    \n",
    "def nlp_preprocess(df: pd.DataFrame, column: str):\n",
    "    df[f'{column}_lemmas'] = df[column].apply(tokenize)\n",
    "    df[f'{column}_bigram'] = df[f'{column}_lemmas'].apply(make_bigram)\n",
    "    \n",
    "def predict(model: lgb.Booster, df: pd.DataFrame, features: List[str]) -> pd.Series:\n",
    "    return model.predict(df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:17<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# df = make_split(summaries_test, prompts_test)\n",
    "df = make_split(summaries_train, prompts_train)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in tqdm(text_columns):\n",
    "    nlp_preprocess(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Create bigram based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "# Create n-gram based features\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(\n",
    "    lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create word based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_words'] = df_train.text_lemmas.str.len()\n",
    "df_train['unique_words'] = df_train.text_lemmas.apply(set).str.len()\n",
    "df_train['unique_ratio'] = df_train.unique_words / df_train.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_lengths'] = df_train.text_lemmas.apply(lambda x: [len(y) for y in x])\n",
    "df_train['word_len_avg'] = df_train.word_lengths.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_len_q10'] = df_train.word_lengths.apply(partial(np.percentile, q=10))\n",
    "df_train['word_len_q90'] = df_train.word_lengths.apply(partial(np.percentile, q=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pos_tag(df_train.text_lemmas[0], tagset='universal')\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(lambda: 0)\n",
    "for _, pos in x:\n",
    "    dd[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pos'] = df_train.text_lemmas.apply(partial(pos_tag, tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counts(tags):\n",
    "    dd = defaultdict(lambda: 0)\n",
    "    for _, pos in tags:\n",
    "        dd[pos] += 1\n",
    "    return dd\n",
    "\n",
    "df_train['pos_counts'] = df_train.pos.apply(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['verb_count'] = df_train.pos_counts.str['VERB'].replace(np.nan, 0)\n",
    "df_train['noun_count'] = df_train.pos_counts.str['NOUN'].replace(np.nan, 0)\n",
    "df_train['adv_count'] = df_train.pos_counts.str['ADV'].replace(np.nan, 0)\n",
    "df_train['adj_count'] = df_train.pos_counts.str['ADJ'].replace(np.nan, 0)\n",
    "df_train['det_count'] = df_train.pos_counts.str['DET'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>det_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>verb_count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.812849</td>\n",
       "      <td>0.681080</td>\n",
       "      <td>0.645030</td>\n",
       "      <td>0.486026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noun_count</th>\n",
       "      <td>0.812849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.603829</td>\n",
       "      <td>0.844499</td>\n",
       "      <td>0.628654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv_count</th>\n",
       "      <td>0.681080</td>\n",
       "      <td>0.603829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533738</td>\n",
       "      <td>0.342280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_count</th>\n",
       "      <td>0.645030</td>\n",
       "      <td>0.844499</td>\n",
       "      <td>0.533738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>det_count</th>\n",
       "      <td>0.486026</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>0.342280</td>\n",
       "      <td>0.566271</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            verb_count  noun_count  adv_count  adj_count  det_count\n",
       "verb_count    1.000000    0.812849   0.681080   0.645030   0.486026\n",
       "noun_count    0.812849    1.000000   0.603829   0.844499   0.628654\n",
       "adv_count     0.681080    0.603829   1.000000   0.533738   0.342280\n",
       "adj_count     0.645030    0.844499   0.533738   1.000000   0.566271\n",
       "det_count     0.486026    0.628654   0.342280   0.566271   1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[['verb_count','noun_count','adv_count','adj_count','det_count']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Create spelling based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 18:08:34 URL:https://codeload.github.com/dwyl/english-words/zip/refs/heads/master [7118481] -> \"master.zip\" [1]\n",
      "replace english-words-master/CONTRIBUTING.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/LICENSE.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/read_english_dictionary.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/scripts/create_json.py? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/scripts/gen.sh? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/word_list_moby_README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/word_list_moby_all_moby_words.icss.yaml? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/word_list_moby_credits.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words_alpha.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words_alpha.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words_dictionary.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace english-words-master/words_dictionary.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget -nv https://github.com/dwyl/english-words/archive/refs/heads/master.zip -O master.zip\n",
    "yes | unzip -q master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./english-words-master/words.txt', 'r') as f:\n",
    "    en_words = [line.strip() for line in f.read().split('\\n')]\n",
    "\n",
    "en_words = set([word for word in en_words if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(col: str) -> Set[str]:\n",
    "    word_sets = df_train[col].apply(set).tolist()\n",
    "    return reduce(or_, word_sets)\n",
    "\n",
    "prompt_words = get_unique_words('prompt_text_lemmas')\n",
    "question_words = get_unique_words('prompt_question_lemmas')\n",
    "title_words = get_unique_words('prompt_title_lemmas')\n",
    "\n",
    "word_set = en_words | prompt_words | question_words | title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commonlit_words.txt', 'w') as f:\n",
    "    f.write('\\n'.join(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dir = words.abspath('en')\n",
    "# Initialise the algorithm\n",
    "metric = Levenshtein()\n",
    "# Index the words from a dictionary\n",
    "# metric.add_from_path('./brown_words.txt')\n",
    "# metric.add_from_path(words.abspath('en'))\n",
    "# metric.add_from_path('./english-words-master/words.txt')\n",
    "metric.add_from_path('./commonlit_words.txt')\n",
    "\n",
    "def get_distances(tokens: List[str], metric: spellwise.algorithms.base.Base) -> List[str]:\n",
    "    distances = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        suggestions = metric.get_suggestions(token)\n",
    "        if suggestions == []:\n",
    "            distance = len(token) if token.isalpha() else 0\n",
    "        else:\n",
    "            distance = suggestions[0]['distance']\n",
    "        distances.append(distance)\n",
    "    return sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.text_lemmas.progress_apply(partial(get_distances, metric=metric))\n",
    "\n",
    "def distance_func(chunk: pd.DataFrame):\n",
    "    return chunk.apply(partial(get_distances, metric=metric))\n",
    "\n",
    "n_jobs = 6\n",
    "# df_chunks = np.array_split(df_train.text_lemmas, n_jobs * 2)\n",
    "\n",
    "# total_edit_distances = Parallel(n_jobs=n_jobs, backend='loky')(delayed(distance_func)(chunk) for chunk in tqdm(df_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:00<00:00, 219697.40it/s]\n"
     ]
    }
   ],
   "source": [
    "count_missing_words = lambda tokens: sum([word not in word_set for word in tokens])\n",
    "df_train['missing_wordcount'] = df_train.text_lemmas.progress_apply(count_missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_lgb_kfold(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst\n",
    "\n",
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    \n",
    "    X_train = features[feature_names].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "    y_train = targets[target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "    train_errors = [calculate_errors(y_train, bst.predict(X_train))]\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors)\n",
    "\n",
    "    return train_metrics, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(f_cols, model_params):\n",
    "    metric_df_content, bst_content = train_lgb_kfold('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb_kfold('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nTrain MCRMSE:\\t   {mcrmse.iloc[0]}')\n",
    "    print(f'Validation MCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_title_unique_bigrams', 'prompt_question_unique_bigrams',\n",
       "       'prompt_text_unique_bigrams', 'text_unique_bigrams',\n",
       "       'text_bigram_overlap', 'question_bigram_overlap', 'text_bigram_ratio',\n",
       "       'text_bigram_diff', 'question_bigram_diff', 'text_bigram_exclusive',\n",
       "       'question_bigram_exclusive', 'n_words', 'unique_words', 'unique_ratio',\n",
       "       'word_len_avg', 'word_len_q10', 'word_len_q90', 'verb_count',\n",
       "       'noun_count', 'adv_count', 'adj_count', 'det_count',\n",
       "       'missing_wordcount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.815664  0.448115  0.347847  train  content\n",
      "mean  0.770489  0.502267  0.391396    val  content\n",
      "mean  0.674074  0.589973  0.464470  train  wording\n",
      "mean  0.525551  0.693794  0.551752    val  wording\n",
      "std   0.005658  0.015824  0.012227  train  content\n",
      "std   0.013918  0.051325  0.035137    val  content\n",
      "std   0.029167  0.024933  0.018364  train  wording\n",
      "std   0.126036  0.128326  0.106174    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.519044030509193\n",
      "Validation MCRMSE: 0.5980305966609889\n",
      "\n",
      "    importance              feature\n",
      "0          399  text_bigram_overlap\n",
      "3          186              n_words\n",
      "1           89  text_unique_bigrams\n",
      "5           75         word_len_avg\n",
      "12          49            det_count\n",
      "2           48         unique_ratio\n",
      "13          39    missing_wordcount\n",
      "4           32         unique_words\n",
      "9           28           noun_count\n",
      "10          27            adv_count\n",
      "8           16           verb_count\n",
      "11          11            adj_count\n",
      "6            1         word_len_q10\n",
      "7            0         word_len_q90\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'objective': 'fair', \n",
    "    'verbose': 0, \n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.08,\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 11,\n",
    "}\n",
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'verb_count','noun_count','adv_count','adj_count','det_count', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.812866  0.451490  0.350580  train  content\n",
      "mean  0.766385  0.506711  0.395081    val  content\n",
      "mean  0.670775  0.592887  0.466304  train  wording\n",
      "mean  0.524203  0.694905  0.552141    val  wording\n",
      "std   0.006487  0.016275  0.012728  train  content\n",
      "std   0.015214  0.052164  0.034654    val  content\n",
      "std   0.031142  0.026552  0.019574  train  wording\n",
      "std   0.125346  0.128070  0.106474    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5221885556899797\n",
      "Validation MCRMSE: 0.6008081678438388\n",
      "\n",
      "    importance              feature\n",
      "0          402  text_bigram_overlap\n",
      "3          186              n_words\n",
      "1          103  text_unique_bigrams\n",
      "5           91         word_len_avg\n",
      "2           52         unique_ratio\n",
      "12          46            det_count\n",
      "10          34            adv_count\n",
      "4           31         unique_words\n",
      "9           27           noun_count\n",
      "8           13           verb_count\n",
      "11          13            adj_count\n",
      "6            2         word_len_q10\n",
      "7            0         word_len_q90\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'verb_count','noun_count','adv_count','adj_count','det_count']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.802370  0.463986  0.359229  train  content\n",
      "mean  0.766108  0.506714  0.392697    val  content\n",
      "mean  0.641367  0.619213  0.486414  train  wording\n",
      "mean  0.512604  0.703468  0.557803    val  wording\n",
      "std   0.001473  0.012892  0.009490  train  content\n",
      "std   0.018112  0.050612  0.034275    val  content\n",
      "std   0.028503  0.029089  0.021428  train  wording\n",
      "std   0.138852  0.140077  0.116270    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.541599318767548\n",
      "Validation MCRMSE: 0.6050906212676062\n",
      "\n",
      "   importance              feature\n",
      "0         447  text_bigram_overlap\n",
      "1         413  text_unique_bigrams\n",
      "3          79    missing_wordcount\n",
      "2          61         unique_ratio\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.801467  0.465053  0.360118  train  content\n",
      "mean  0.767347  0.505449  0.391702    val  content\n",
      "mean  0.636399  0.623498  0.489565  train  wording\n",
      "mean  0.522515  0.696076  0.551708    val  wording\n",
      "std   0.001735  0.013392  0.009936  train  content\n",
      "std   0.018237  0.051704  0.035528    val  content\n",
      "std   0.028995  0.029648  0.021851  train  wording\n",
      "std   0.136099  0.137848  0.113507    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5442754400162207\n",
      "Validation MCRMSE: 0.6007627017618824\n",
      "\n",
      "   importance              feature\n",
      "0         468  text_bigram_overlap\n",
      "1         432  text_unique_bigrams\n",
      "2         100    missing_wordcount\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.809939  0.455013  0.353506  train  content\n",
      "mean  0.768888  0.503966  0.392758    val  content\n",
      "mean  0.656957  0.605207  0.474982  train  wording\n",
      "mean  0.519213  0.698588  0.554200    val  wording\n",
      "std   0.006624  0.016646  0.012717  train  content\n",
      "std   0.010546  0.048736  0.033313    val  content\n",
      "std   0.033915  0.029978  0.022120  train  wording\n",
      "std   0.129986  0.131704  0.110075    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5301098046221431\n",
      "Validation MCRMSE: 0.6012768396782324\n",
      "\n",
      "   importance              feature\n",
      "0         426  text_bigram_overlap\n",
      "3         232              n_words\n",
      "1         126  text_unique_bigrams\n",
      "5         107         word_len_avg\n",
      "2          57         unique_ratio\n",
      "4          37         unique_words\n",
      "6          12         word_len_q10\n",
      "7           3         word_len_q90\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            r2      rmse       mae    set   target\n",
      "mean  0.813635  0.450579  0.349997  train  content\n",
      "mean  0.773873  0.498526  0.388436    val  content\n",
      "mean  0.662685  0.600267  0.472171  train  wording\n",
      "mean  0.520081  0.698155  0.554405    val  wording\n",
      "std   0.005523  0.015899  0.011967  train  content\n",
      "std   0.010618  0.048704  0.033635    val  content\n",
      "std   0.030107  0.027383  0.020360  train  wording\n",
      "std   0.126758  0.129643  0.108927    val  wording\n",
      "\n",
      "Train MCRMSE:\t   0.5254230479083919\n",
      "Validation MCRMSE: 0.5983402765159832\n",
      "\n",
      "   importance              feature\n",
      "0         403  text_bigram_overlap\n",
      "3         230              n_words\n",
      "1         124  text_unique_bigrams\n",
      "5          85         word_len_avg\n",
      "8          64    missing_wordcount\n",
      "2          44         unique_ratio\n",
      "4          41         unique_words\n",
      "6           9         word_len_q10\n",
      "7           0         word_len_q90\n"
     ]
    }
   ],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "\tContent scores\n",
      "         r2      rmse       mae\n",
      "0  0.808225  0.456969  0.354846\n",
      "\n",
      "-----------------------------------\n",
      "\tWording scores\n",
      "         r2      rmse       mae\n",
      "0  0.652207  0.610957  0.480325\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{\"-\"*35}\\n\\tContent scores')\n",
    "pprint(metric_df_content)\n",
    "print(f'\\n{\"-\"*35}\\n\\tWording scores')\n",
    "pprint(metric_df_wording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data MCRMSE: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.533963\n",
       "Name: rmse, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Full data MCRMSE: ')\n",
    "(metric_df_content.rmse + metric_df_wording.rmse) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2b01100d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_content.save_model(content_model)\n",
    "bst_wording.save_model(wording_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
