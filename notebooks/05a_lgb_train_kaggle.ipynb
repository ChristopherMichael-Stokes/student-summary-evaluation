{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and nltk downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "download_dir = f'{os.getcwd()}/nltk_data'\n",
    "os.environ['NLTK_DATA'] = download_dir\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir=download_dir)\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('wordnet', download_dir=download_dir)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=download_dir)\n",
    "nltk.download('universal_tagset', download_dir=download_dir)\n",
    "nltk.download('words', download_dir=download_dir)\n",
    "nltk.data.path.append(download_dir)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ngrams\n",
    "import spellwise\n",
    "from spellwise import Levenshtein\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "from functools import partial, reduce\n",
    "from operator import or_\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Union, List, Tuple, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Fix nltk installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "yes | unzip -q nltk_data/corpora/wordnet.zip -d nltk_data/corpora/\n",
    "ls nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "tqdm.pandas()\n",
    "\n",
    "data_dir = Path('../data/commonlit-evaluate-student-summaries')\n",
    "\n",
    "sample_submission = data_dir / 'sample_submission.csv'\n",
    "summaries_train = data_dir / 'summaries_train.csv'\n",
    "summaries_test = data_dir / 'summaries_test.csv'\n",
    "prompts_train = data_dir / 'prompts_train.csv'\n",
    "prompts_test = data_dir / 'prompts_test.csv'\n",
    "\n",
    "content_model = '../data/models/content.txt'\n",
    "wording_model = '../data/models/wording.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(summaries_path: Path, prompts_path: Path, dtype_backend: Optional[str] = 'pyarrow') -> pd.DataFrame:\n",
    "    summaries_df = pd.read_csv(summaries_path)#, dtype_backend=dtype_backend)\n",
    "    prompts_df = pd.read_csv(prompts_path)#, dtype_backend=dtype_backend)\n",
    "    df = pd.merge(summaries_df, prompts_df, how='left', on='prompt_id')\n",
    "    df.fillna('')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return [lemmatiser.lemmatize(tok.lower()) for tok in word_tokenize(text) if tok.isalnum() and tok not in stop_words]\n",
    "\n",
    "\n",
    "def make_bigram(tokens: List[str]) -> Set[str]:\n",
    "    return set(ngrams(tokens, 2))\n",
    "\n",
    "\n",
    "def clear_stopwords(column: pd.Series, idx: int) -> Union[List[str], List[str], List[str]]:\n",
    "    tokens = [tok.lower() for tok in word_tokenize(column.iloc[idx]) if tok.isalnum()]\n",
    "    cleared_stopwords = [tok for tok in tokens if tok not in stop_words]\n",
    "    lemmas = [lemmatiser.lemmatize(tok) for tok in cleared_stopwords]\n",
    "    bigram = set(ngrams(lemmas, 2))\n",
    "\n",
    "    return tokens, cleared_stopwords, lemmas, bigram\n",
    "    \n",
    "    \n",
    "def nlp_preprocess(df: pd.DataFrame, column: str):\n",
    "    df[f'{column}_lemmas'] = df[column].apply(tokenize)\n",
    "    df[f'{column}_bigram'] = df[f'{column}_lemmas'].apply(make_bigram)\n",
    "    \n",
    "def predict(model: lgb.Booster, df: pd.DataFrame, features: List[str]) -> pd.Series:\n",
    "    return model.predict(df[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# df = make_split(summaries_test, prompts_test)\n",
    "df = make_split(summaries_train, prompts_train)\n",
    "\n",
    "# Make n-grams for all text columns\n",
    "text_columns = ['prompt_title', 'prompt_question', 'prompt_text', 'text']\n",
    "for column in tqdm(text_columns):\n",
    "    nlp_preprocess(df, column)\n",
    "    df[f'{column}_unique_bigrams'] = df[f'{column}_bigram'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Create bigram based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "# Create n-gram based features\n",
    "df_train['text_bigram_overlap'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_overlap'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] & row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['text_bigram_ratio'] = df_train['text_unique_bigrams'] / (df_train['prompt_text_unique_bigrams'])\n",
    "\n",
    "df_train['text_bigram_diff'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_diff'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[1] - row[0]), axis=1) / df_train.text_unique_bigrams\n",
    "\n",
    "df_train['text_bigram_exclusive'] = df_train[['prompt_text_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams\n",
    "df_train['question_bigram_exclusive'] = df_train[['prompt_question_bigram', 'text_bigram']].apply(lambda row: len(row[0] ^ row[1]), axis=1) / df_train.text_unique_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create word based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['n_words'] = df_train.text_lemmas.str.len()\n",
    "df_train['unique_words'] = df_train.text_lemmas.apply(set).str.len()\n",
    "df_train['unique_ratio'] = df_train.unique_words / df_train.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_lengths'] = df_train.text_lemmas.apply(lambda x: [len(y) for y in x])\n",
    "df_train['word_len_avg'] = df_train.word_lengths.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_len_q10'] = df_train.word_lengths.apply(partial(np.percentile, q=10))\n",
    "df_train['word_len_q90'] = df_train.word_lengths.apply(partial(np.percentile, q=90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pos_tag(df_train.text_lemmas[0], tagset='universal')\n",
    "from collections import defaultdict\n",
    "\n",
    "dd = defaultdict(lambda: 0)\n",
    "for _, pos in x:\n",
    "    dd[pos] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pos'] = df_train.text_lemmas.apply(partial(pos_tag, tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_counts(tags):\n",
    "    dd = defaultdict(lambda: 0)\n",
    "    for _, pos in tags:\n",
    "        dd[pos] += 1\n",
    "    return dd\n",
    "\n",
    "df_train['pos_counts'] = df_train.pos.apply(pos_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['verb_count'] = df_train.pos_counts.str['VERB'].replace(np.nan, 0)\n",
    "df_train['noun_count'] = df_train.pos_counts.str['NOUN'].replace(np.nan, 0)\n",
    "df_train['adv_count'] = df_train.pos_counts.str['ADV'].replace(np.nan, 0)\n",
    "df_train['adj_count'] = df_train.pos_counts.str['ADJ'].replace(np.nan, 0)\n",
    "df_train['det_count'] = df_train.pos_counts.str['DET'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['verb_count','noun_count','adv_count','adj_count','det_count']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Create spelling based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -nv https://github.com/dwyl/english-words/archive/refs/heads/master.zip -O master.zip\n",
    "yes | unzip -q master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./english-words-master/words.txt', 'r') as f:\n",
    "    en_words = [line.strip() for line in f.read().split('\\n')]\n",
    "\n",
    "en_words = set([word for word in en_words if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(col: str) -> Set[str]:\n",
    "    word_sets = df_train[col].apply(set).tolist()\n",
    "    return reduce(or_, word_sets)\n",
    "\n",
    "prompt_words = get_unique_words('prompt_text_lemmas')\n",
    "question_words = get_unique_words('prompt_question_lemmas')\n",
    "title_words = get_unique_words('prompt_title_lemmas')\n",
    "\n",
    "word_set = en_words | prompt_words | question_words | title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commonlit_words.txt', 'w') as f:\n",
    "    f.write('\\n'.join(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dir = words.abspath('en')\n",
    "# Initialise the algorithm\n",
    "metric = Levenshtein()\n",
    "# Index the words from a dictionary\n",
    "# metric.add_from_path('./brown_words.txt')\n",
    "# metric.add_from_path(words.abspath('en'))\n",
    "# metric.add_from_path('./english-words-master/words.txt')\n",
    "metric.add_from_path('./commonlit_words.txt')\n",
    "\n",
    "def get_distances(tokens: List[str], metric: spellwise.algorithms.base.Base) -> List[str]:\n",
    "    distances = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        suggestions = metric.get_suggestions(token)\n",
    "        if suggestions == []:\n",
    "            distance = len(token) if token.isalpha() else 0\n",
    "        else:\n",
    "            distance = suggestions[0]['distance']\n",
    "        distances.append(distance)\n",
    "    return sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.text_lemmas.progress_apply(partial(get_distances, metric=metric))\n",
    "\n",
    "def distance_func(chunk: pd.DataFrame):\n",
    "    return chunk.apply(partial(get_distances, metric=metric))\n",
    "\n",
    "n_jobs = 7\n",
    "df_chunks = np.array_split(df_train.text_lemmas, n_jobs * 2)\n",
    "\n",
    "total_edit_distances = Parallel(n_jobs=n_jobs, backend='loky')(delayed(distance_func)(chunk) for chunk in tqdm(df_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_missing_words = lambda tokens: sum([word not in word_set for word in tokens])\n",
    "df_train['missing_wordcount'] = df_train.text_lemmas.progress_apply(count_missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=np.number)\n",
    "target_columns = ['content', 'wording']\n",
    "feature_columns = [col for col in numeric_features if col not in target_columns]\n",
    "\n",
    "targets = numeric_features[target_columns]\n",
    "features = numeric_features[feature_columns]\n",
    "prompt_group = pd.Categorical(df['prompt_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(y, y_pred):\n",
    "    return {\n",
    "        'r2': r2_score(y, y_pred),\n",
    "        'rmse': sqrt(mean_squared_error(y, y_pred)),\n",
    "        'mae': mean_absolute_error(y, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_lgb_kfold(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=prompt_group.unique().size)\n",
    "    assert group_kfold.get_n_splits(features, targets, prompt_group) == len(prompt_group.unique())\n",
    "\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    for i, (train_index, test_index) in enumerate(group_kfold.split(features, targets, prompt_group)):\n",
    "        # print(f'Fold {i}')\n",
    "        # print(f'\\tTest prompt: {df.iloc[test_index].prompt_title.unique().tolist()}')\n",
    "\n",
    "        X_train = features[feature_names].iloc[train_index].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "        y_train = targets.iloc[train_index][target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "        X_val = features[feature_names].iloc[test_index]\n",
    "        y_val = targets.iloc[test_index][target]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, y_val)\n",
    "        bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "        train_errors.append(calculate_errors(y_train, bst.predict(X_train)))\n",
    "        val_errors.append(calculate_errors(y_val, bst.predict(X_val)))\n",
    "\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors).describe()\n",
    "    train_metrics['set'] = 'train'\n",
    "    val_metrics = pd.DataFrame.from_records(val_errors).describe()\n",
    "    val_metrics['set'] = 'val'\n",
    "    metric_df = pd.concat([train_metrics, val_metrics])\n",
    "\n",
    "    return metric_df, bst\n",
    "\n",
    "def train_lgb(\n",
    "        target: str, \n",
    "        prompt_group: pd.DataFrame, \n",
    "        features: pd.DataFrame, \n",
    "        targets: pd.DataFrame, \n",
    "        feature_names: List[str],\n",
    "        model_params: dict) -> Tuple[pd.DataFrame, lgb.LGBMRegressor]:\n",
    "    \n",
    "    \n",
    "    X_train = features[feature_names].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "    y_train = targets[target].convert_dtypes(dtype_backend='numpy_nullable')\n",
    "\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    bst = lgb.train(model_params, train_data, )#, feval=[r2_score, mean_absolute_error])\n",
    "\n",
    "    train_errors = [calculate_errors(y_train, bst.predict(X_train))]\n",
    "    train_metrics = pd.DataFrame.from_records(train_errors)\n",
    "\n",
    "    return train_metrics, bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation(f_cols, model_params):\n",
    "    metric_df_content, bst_content = train_lgb_kfold('content', prompt_group, features, targets, f_cols, model_params)\n",
    "    metric_df_wording, bst_wording = train_lgb_kfold('wording', prompt_group, features, targets, f_cols, model_params)\n",
    "\n",
    "    metric_df_content['target'] = 'content'\n",
    "    metric_df_wording['target'] = 'wording'\n",
    "    metric_df = pd.concat([metric_df_content, metric_df_wording])\n",
    "    metric_df = metric_df.loc[['mean', 'std']]\n",
    "    print(metric_df)\n",
    "\n",
    "    mcrmse = (metric_df.loc[metric_df.target=='content', 'rmse'] + metric_df.loc[metric_df.target=='wording', 'rmse']) / 2\n",
    "    print(f'\\nTrain MCRMSE:\\t   {mcrmse.iloc[0]}')\n",
    "    print(f'Validation MCRMSE: {mcrmse.iloc[1]}\\n')\n",
    "\n",
    "    importance = pd.DataFrame({\n",
    "    'importance': bst_wording.feature_importance(),\n",
    "    'feature': bst_wording.feature_name()}).sort_values(by='importance', ascending=False)\n",
    "    print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'objective': 'fair', \n",
    "    'verbose': 0, \n",
    "    'force_col_wise': True,\n",
    "    'learning_rate': 0.08,\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': 11,\n",
    "}\n",
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'verb_count','noun_count','adv_count','adj_count','det_count', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cols = ['text_bigram_overlap', 'text_unique_bigrams', 'unique_ratio', \n",
    "          'n_words', 'unique_words', 'word_len_avg', 'word_len_q10', 'word_len_q90',\n",
    "          'missing_wordcount']\n",
    "\n",
    "eval_validation(f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_content, bst_content = train_lgb('content', prompt_group, features, targets, f_cols, model_params)\n",
    "metric_df_wording, bst_wording = train_lgb('wording', prompt_group, features, targets, f_cols, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{\"-\"*35}\\n\\tContent scores')\n",
    "pprint(metric_df_content)\n",
    "print(f'\\n{\"-\"*35}\\n\\tWording scores')\n",
    "pprint(metric_df_wording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full data MCRMSE: ')\n",
    "(metric_df_content.rmse + metric_df_wording.rmse) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_content.save_model(content_model)\n",
    "bst_wording.save_model(wording_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
